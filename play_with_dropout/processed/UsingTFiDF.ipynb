{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import data\n",
    "import config\n",
    "import time\n",
    "from dateparser.search import search_dates\n",
    "import pandas as pd\n",
    "\n",
    "stop_word = ['?', '.']\n",
    "def clearing_word(word):\n",
    "    word = re.sub('\\x8e', 'é', word)\n",
    "    word = re.sub('\\x88', 'à', word)\n",
    "    word = re.sub('\\x9d', 'ù', word)\n",
    "    word = re.sub('\\x8f', 'è', word)\n",
    "    word = re.sub('\\x9e', 'û', word)\n",
    "    word = re.sub('\\x90', 'ê', word)\n",
    "    word = re.sub('\\x99', 'ô', word)\n",
    "    word = re.sub('\\x94', 'î', word)\n",
    "   # word = re.sub('\\x8f', 'è', word)\n",
    "    word = re.sub('\\x8d', 'ç', word)\n",
    "    word = re.sub('õ', '', word)\n",
    "    word = re.sub('Ê', '', word)\n",
    "    word = re.sub('[?,.,!, \\,,  %]', '', word)\n",
    "    if word == 'û' or word == 'v' or word == 'é':\n",
    "        word = ''\n",
    "    if word ==\"2017êles\":\n",
    "        word = \"2017\"\n",
    "    if \"ênox\" in word:\n",
    "        word =\"nox\"\n",
    "    return word\n",
    "\n",
    "def clearing(pharse):\n",
    "    \"\"\"\n",
    "    Arg: Data is a list of questions or answers\n",
    "    Return: clean questions et answers \n",
    "    \"\"\"\n",
    "    #clean_data = []\n",
    "    pharses =[]\n",
    "    for word in pharse.strip().lower().split(' '):\n",
    "       # line= data[i].lower().split(' ')        \n",
    "        #for word in line:\n",
    "        word = clearing_word(word)\n",
    "        if word not in stop_word and len(word) !=0:       \n",
    "            pharses.append(word)        \n",
    "    return ' '.join(pharses)\n",
    "\n",
    "def prepareData(PAIRS):\n",
    "    pairs_trains, pairs_tests = [], []\n",
    "   # PAIRS.extend(pairs1)\n",
    "    index_train = random.sample(range(len(PAIRS)), int(len(PAIRS)*0.80))\n",
    "    for i in range(len(PAIRS)):\n",
    "        if i in index_train:\n",
    "            pairs_trains.append(PAIRS[i])\n",
    "        else:\n",
    "            pairs_tests.append(PAIRS[i])\n",
    "    print(\"Read %s sentence pairs of training set\" % len(pairs_trains))\n",
    "    print(\"Read %s sentence pairs of test set\" % len(pairs_tests))\n",
    "    return pairs_trains, pairs_tests\n",
    "\n",
    "def get_all_convos():\n",
    "    convos = []\n",
    "    file0 = 'convos27juin.txt'\n",
    "    #file1 = 'chatbot_tout_corpus.txt'\n",
    "    #file2 = 'chatbot_tout_corpus_13juin.txt'\n",
    "    #file3 = 'convos_120_nettoye.txt' # cette fichier viens d'etre ajouter\n",
    "    liste_file = [file0]\n",
    "    for file in liste_file:\n",
    "        with open(file) as f:\n",
    "            i=0\n",
    "            for line in f:\n",
    "                if i%2==0:\n",
    "                    question = clearing(line)\n",
    "                    if '++++' in question:\n",
    "                         question = question[9:]\n",
    "                else:\n",
    "                    answer = clearing(line)\n",
    "                    if '++++' in answer:\n",
    "                        answer = answer[9:]\n",
    "                    convos.append([question, answer])\n",
    "                i+=1\n",
    "        f.close()\n",
    "    return convos\n",
    "\n",
    "def simulation_pairs():\n",
    "    DIRECT_VARIABLE = ['vitesse', 'batterie', 'position', 'km']\n",
    "    TIME = []\n",
    "    GEOGRAPHY = ['Lyon', 'hors lyon', 'paris', 'l\\'étranger','france' , 'hors la france']\n",
    "    BORDE = {'MAX': ['plus grand', 'plus grande', 'plus grands', \n",
    "         'plus grandes',  'moin faible', 'moins faibles',\n",
    "         'plus vite', 'max', 'maximum', 'maximal', 'maximaux', 'maximale',\n",
    "        'plus haut', 'plus haute', 'plus hautes', 'plus hauts',  \n",
    "        'plus élevé', 'plus élevée', 'plus élevés', 'plus élevées'], \n",
    "         'MIN':['moins grand', 'moins grande', 'moins grands','moins grandes',\n",
    "        'plus petit', 'plus petite', 'plus petits', 'plus petites','plus faible', 'plus faibles', \n",
    "         'min', 'minimal', 'minimale', 'minimales', 'minimaux','moins vite', \n",
    "         'moins élevé', 'moins élevée', 'moins élevés', 'moins élevées'],\n",
    "        'MOYENNE': ['moyenne', 'moyen', 'moyennement']\n",
    "        }\n",
    "    COMPLEX_ANALYSIS = ['problème', 'problèmes', 'erreurs', 'erreur', 'danger']\n",
    "    PAIRS = []\n",
    "    question_for_simulation = ['quels véhicules ont', 'quel véhicule', 'quelle voiture', 'quelles voitures']\n",
    "    reponse_for_simulation = 'véhicule'\n",
    "    for borde in BORDE:\n",
    "        for word in BORDE[borde]:\n",
    "            for direct_variable in DIRECT_VARIABLE:\n",
    "                for question_f_s in question_for_simulation:\n",
    "                    if direct_variable !='position':\n",
    "                        question= question_f_s+' '+direct_variable + ' '+ word\n",
    "                        key0 = '#id#'\n",
    "                        key1 = '#'+direct_variable+'#'\n",
    "                        key2 = '#'+borde +'#'\n",
    "                        reponse = reponse_for_simulation +' '+key0+ ' '+ key1+ ' '+ key2\n",
    "                        PAIRS.append([question,reponse])\n",
    "\n",
    "    question_for_simulation = ['', 'quel véhicule a', 'quelle voiture a', 'quelles voitures ont', 'il y a']\n",
    "    reponse_for_simulation = 'véhicule' \n",
    "    for direct_variable in DIRECT_VARIABLE:\n",
    "        for complex_analysis in COMPLEX_ANALYSIS:\n",
    "            for question_f_s in question_for_simulation:\n",
    "                #if direct_variable !='position':\n",
    "                      question= question_f_s+' '+complex_analysis + ' '+ direct_variable\n",
    "                      key0 = '#id#'\n",
    "                      key1 = '#'+direct_variable+'#'\n",
    "                      key2 = '#'+ complex_analysis +'#'\n",
    "                      reponse = reponse_for_simulation +' '+key0+ ' '+ key2 +' '+ key1\n",
    "                      PAIRS.append([question,reponse])\n",
    "    return PAIRS\n",
    "\n",
    "\n",
    "def clear_pairs_trains(pairs_trains):\n",
    "    questions = []\n",
    "    index = []\n",
    "    for i in range(len(pairs_trains)):\n",
    "        if  pairs_trains[i][0] not in questions:\n",
    "            index.append(i)\n",
    "            questions.append(pairs_trains[i][0])\n",
    "    return [pairs_trains[i] for i in index]\n",
    "\n",
    "def tf_idf(word, doc, méthode, QUESTION_TO_TRAINS):\n",
    "    \n",
    "    if méthode  == 'normal':\n",
    "        tf, df =0, 0\n",
    "        for token in doc.split():\n",
    "            if word==token:\n",
    "                tf+=1\n",
    "        for doc_ex in QUESTION_TO_TRAINS:\n",
    "            for  token  in doc_ex.split():\n",
    "                if token == word:\n",
    "                    df+=1\n",
    "                    break\n",
    "        if df>0:\n",
    "            return tf*math.log(len(QUESTION_TO_TRAINS)/(df), 10)\n",
    "        else:\n",
    "            return 0\n",
    "    if méthode == 'probabiliste':\n",
    "        tf, df =0, 0\n",
    "        for token in doc.split():\n",
    "            if word==token:\n",
    "                tf+=1\n",
    "        for doc_ex in QUESTION_TO_TRAINS:\n",
    "            for  token  in doc_ex.split():\n",
    "                if token == word:\n",
    "                    df+=1\n",
    "                    break\n",
    "        if df>0:\n",
    "            return tf*math.log((len(QUESTION_TO_TRAINS)-df)/(df), 10)\n",
    "        else:\n",
    "            return 0\n",
    "    if méthode == 'lissé':\n",
    "        tf, df =0, 0\n",
    "        for token in doc.split():\n",
    "            if word==token:\n",
    "                tf+=1\n",
    "        for doc_ex in QUESTION_TO_TRAINS:\n",
    "            for  token  in doc_ex.split():\n",
    "                if token == word:\n",
    "                    df+=1\n",
    "                    break\n",
    "        if df>0:\n",
    "            return tf*(1+math.log(len(QUESTION_TO_TRAINS)/df, 10))\n",
    "        else:\n",
    "            return tf\n",
    "    if méthode == 'probabiliste_lissé':\n",
    "        tf, df =0, 0\n",
    "        for token in doc.split():\n",
    "            if word==token:\n",
    "                tf+=1\n",
    "        for doc_ex in QUESTION_TO_TRAINS:\n",
    "            for  token  in doc_ex.split():\n",
    "                if token == word:\n",
    "                    df+=1\n",
    "                    break\n",
    "        if df>0:\n",
    "            return tf*(1+math.log((len(QUESTION_TO_TRAINS)-df)/(df), 10))\n",
    "        else:\n",
    "            return tf\n",
    "    if méthode =='Lucene':\n",
    "        tf, df =0, 0\n",
    "        for token in doc.split():\n",
    "            if word==token:\n",
    "                tf+=1\n",
    "        for doc_ex in QUESTION_TO_TRAINS:\n",
    "            for  token  in doc_ex.split():\n",
    "                if token == word:\n",
    "                    df+=1\n",
    "                    break\n",
    "        idf = 1 + math.log(len(QUESTION_TO_TRAINS)/(df+1), 10)\n",
    "        tf = tf/len(QUESTION_TO_TRAINS)\n",
    "        return tf, idf\n",
    "    if méthode =='count':\n",
    "        count_word = 0\n",
    "        if word not in doc.split():\n",
    "            return 0\n",
    "        else:\n",
    "            for doc_ex in QUESTION_TO_TRAINS:\n",
    "                if word in doc_ex.split():\n",
    "                    count_word +=1\n",
    "            return count_word/len(QUESTION_TO_TRAINS)\n",
    "    if méthode == 'inverse_tfidf':\n",
    "        tf, idf = 0,0\n",
    "        for token in doc.split():\n",
    "            if token == word:\n",
    "                tf+=1\n",
    "        for doc_ex in QUESTION_TO_TRAINS:\n",
    "            if word in doc_ex.split():\n",
    "                idf+=1\n",
    "        idf = math.log(len(QUESTION_TO_TRAINS)/(idf+1), 10)\n",
    "        if tf*idf>0:\n",
    "            return 1/(tf*idf)\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "\n",
    "    \n",
    "                    \n",
    "        \n",
    "        \n",
    "def dic_doc_score(doc, méthode):\n",
    "    \"\"\"\n",
    "    we say that a word in KEY_LIST is 2 times more important than other word\"\"\"\n",
    "    \n",
    "    if méthode != 'Lucene':\n",
    "        return {word: tf_idf(word,doc, méthode, QUESTION_TO_TRAINS) for word in doc.split()}\n",
    "    else:\n",
    "        dictionnary ={}\n",
    "        for word in doc.split():\n",
    "            tf, idf = tf_idf(word,doc, méthode, QUESTION_TO_TRAINS)\n",
    "            if word in KEY_LIST:\n",
    "                dictionnary[word] = idf**2*2*tf\n",
    "            else:\n",
    "                dictionnary[word] =idf**2*tf\n",
    "        return dictionnary\n",
    "\n",
    "def score_by_new_doc(new_doc, doc, DICT, méthode):\n",
    "    if méthode != 'Lucene':\n",
    "        DICT_of_methode = DICT[méthode]\n",
    "        score = 0\n",
    "        for word in new_doc.split():\n",
    "            if word in DICT_of_methode[doc]:\n",
    "                 score += DICT_of_methode[doc][word]\n",
    "        return score\n",
    "    else:\n",
    "        DICT_of_methode = DICT[méthode]\n",
    "        score = 0\n",
    "        for word in new_doc.split():\n",
    "            if word in DICT_of_methode[doc]:\n",
    "                 score += DICT_of_methode[doc][word]\n",
    "        coord = 1.5*len([word for word in new_doc if word in doc])\n",
    "        return  coord*score\n",
    "    \n",
    "def get_key_list(pairs_trains):\n",
    "    KEY_LIST = []\n",
    "    for pair in pairs_trains:\n",
    "        for word in pair[1].split():\n",
    "            if '#' in word and word not in KEY_LIST:\n",
    "                KEY_LIST.append(word)\n",
    "    return KEY_LIST\n",
    "        \n",
    "\n",
    "def find_right_index(new_doc,DICT, méthode):\n",
    "    index=0\n",
    "    biggest_score = score_by_new_doc(new_doc, QUESTION_TO_TRAINS[0],DICT, méthode)\n",
    "    for i in range(len(QUESTION_TO_TRAINS)):\n",
    "        if score_by_new_doc(new_doc, QUESTION_TO_TRAINS[i], DICT, méthode) >biggest_score:\n",
    "            biggest_score = score_by_new_doc(new_doc, QUESTION_TO_TRAINS[i],DICT, méthode)\n",
    "            index = i\n",
    "    return index\n",
    "\n",
    "def test(DICT, méthode, train =False):\n",
    "    if not train:\n",
    "        index_to_print = random.sample(range(len(pairs_tests)), 20)\n",
    "        loss_total = 0\n",
    "        for i in range(len(pairs_tests)):\n",
    "            index = find_right_index(pairs_tests[i][0], DICT, méthode)\n",
    "            loss = _evaluate_by_right_word(pairs_trains[index][1], pairs_tests[i][1])\n",
    "            loss_total +=loss\n",
    "            if i in index_to_print:\n",
    "                print(pairs_tests[i])\n",
    "                print(pairs_trains[index][1], '  with accuracy ', 1-loss)\n",
    "        print('-'*80)\n",
    "        print('ACCURACY=', 1-loss_total/len(pairs_tests))\n",
    "    if  train:\n",
    "        loss_total = 0\n",
    "        index_to_print = random.sample(range(len(pairs_trains)), 20)   \n",
    "        for i in range(len(pairs_trains)):\n",
    "            index = find_right_index(pairs_trains[i][0], DICT, méthode)\n",
    "            loss = _evaluate_by_right_word(pairs_trains[index][1], pairs_trains[i][1])\n",
    "            loss_total +=loss\n",
    "            if loss >0:\n",
    "                print(pairs_trains[i])\n",
    "                print(pairs_trains[index][1], '  with accuracy ', 1-loss)\n",
    "        print('-'*80)\n",
    "        print('ACCURACY=', 1-loss_total/len(pairs_trains))\n",
    "\n",
    "def test_without_print(DICT, méthode, train =False):\n",
    "    if not train:\n",
    "        loss_total = 0\n",
    "        for i in range(len(pairs_tests)):\n",
    "            index = find_right_index(pairs_tests[i][0], DICT, méthode)\n",
    "            loss = _evaluate_by_right_word(pairs_trains[index][1], pairs_tests[i][1])\n",
    "            loss_total +=loss\n",
    "        print('-'*80)\n",
    "        print('ACCURACY=', 1-loss_total/len(pairs_tests))\n",
    "    if  train:\n",
    "        loss_total = 0\n",
    "        index_to_print = random.sample(range(len(pairs_trains)), 20)   \n",
    "        for i in range(len(pairs_trains)):\n",
    "            index = find_right_index(pairs_trains[i][0], DICT, méthode)\n",
    "            loss = _evaluate_by_right_word(pairs_trains[index][1], pairs_trains[i][1])\n",
    "            loss_total +=loss\n",
    "        print('-'*80)\n",
    "        print('ACCURACY=', 1-loss_total/len(pairs_trains))\n",
    "        \n",
    "        \n",
    "def chat(méthode):\n",
    "    print('Bonjour, C\\'est le bot d\\'Avicen, posez vos questions, s\\'il vous plaît!')\n",
    "    path = 'processed/chat_tfidf.txt'\n",
    "    with open(path, 'w') as f:\n",
    "        while True:\n",
    "            line = str(input('Vous: '))\n",
    "            line = line.lower().strip()\n",
    "            if len(line) !=0:\n",
    "                index= find_right_index(line, DICT, méthode)\n",
    "                print('bot: ', pairs_trains[index][1])\n",
    "                f.write('VOUS ++++ '+line+'\\n')\n",
    "                f.write('BOT ++++ '+pairs_trains[index][1]+'\\n')\n",
    "            \n",
    "            else:\n",
    "                f.close()\n",
    "                break \n",
    "def precision_recall(lstcomp, lstref) :\n",
    "    card_intersec = 0.0 # force à utiliser la division non entière\n",
    "    for t in set(lstcomp) :\n",
    "        card_intersec += min(lstref.count(t), lstcomp.count(t))\n",
    "    precision = card_intersec/len(lstcomp)\n",
    "    rappel = card_intersec/len(lstref)\n",
    "    return precision, rappel\n",
    "\n",
    "    \n",
    "        \n",
    "def test_with_precision_recall(DICT, méthode, train = False):\n",
    "    \n",
    "    if not train:\n",
    "        total_precision, total_recall = 0, 0\n",
    "        for i in range(len(pairs_tests)):\n",
    "            index = find_right_index(pairs_tests[i][0], DICT, méthode)\n",
    "            precision, recall = precision_recall(pairs_tests[i][1], pairs_trains[index][1])\n",
    "            total_precision  += precision\n",
    "            total_recall     += recall\n",
    "        print('-'*80)\n",
    "        total_precision = total_precision/len(pairs_tests)\n",
    "        total_recall = total_recall/len(pairs_tests)      \n",
    "        F = 2*total_precision*total_recall/(total_precision+total_recall)\n",
    "        print('PRECISION = {:.5f},  RECALL = {:.5f}, F_MESURE = {:.3f} METHODE = {}'\\\n",
    "              .format(total_precision, total_recall,F, méthode))\n",
    "    if  train:\n",
    "        total_precision, total_recall = 0, 0\n",
    "        for i in range(len(pairs_trains)):          \n",
    "            index = find_right_index(pairs_trains[i][0], DICT, méthode)\n",
    "            precision, recall = precision_recall(pairs_trains[i][1], pairs_trains[index][1])\n",
    "            total_precision  += precision\n",
    "            total_recall     += recall\n",
    "        print('-'*80)\n",
    "        total_precision = total_precision/len(pairs_trains)\n",
    "        total_recall = total_recall/len(pairs_trains)\n",
    "        F = 2*total_precision*total_recall/(total_precision+total_recall)\n",
    "        print('PRECISION = {:.5f},  RECALL = {:.5f}, F_MESURE = {:.3f} METHODE = {}'\\\n",
    "              .format(total_precision, total_recall, F, méthode))\n",
    "        \n",
    "            \n",
    "def dict_of_all_score(METHODE):\n",
    "    \"\"\"\n",
    "    return score of all word in all doc\"\"\"\n",
    "    DICT = {}\n",
    "    for méthode in METHODE:\n",
    "        dict_for_this_method ={}\n",
    "        for doc in QUESTION_TO_TRAINS:\n",
    "            dict_for_this_method[doc]=dic_doc_score(doc, méthode)\n",
    "        DICT[méthode] = dict_for_this_method\n",
    "    return DICT\n",
    "        \n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    try:\n",
    "        pairs_trains\n",
    "    except NameError :\n",
    "        pairs1 = get_all_convos()\n",
    "        #pairs2 = simulation_pairs()\n",
    "        pairs_trains, pairs_tests = prepareData(pairs1)\n",
    "        #pairs_trains = clear_pairs_trains(pairs_trains)\n",
    "        KEY_LIST = get_key_list(pairs_trains)\n",
    "        QUESTION_TO_TRAINS = []\n",
    "        for pairs in pairs_trains:\n",
    "            QUESTION_TO_TRAINS.append(pairs[0])\n",
    "        METHODE = ['normal','probabiliste', 'lissé', 'probabiliste_lissé', 'Lucene', 'count', 'inverse_tfidf']\n",
    "        DICT = dict_of_all_score(METHODE)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "PRECISION = 0.99787,  RECALL = 0.99379, F_MESURE = 0.996 METHODE = normal\n",
      "--------------------------------------------------------------------------------\n",
      "PRECISION = 0.99787,  RECALL = 0.99379, F_MESURE = 0.996 METHODE = probabiliste\n",
      "--------------------------------------------------------------------------------\n",
      "PRECISION = 0.99787,  RECALL = 0.99379, F_MESURE = 0.996 METHODE = lissé\n",
      "--------------------------------------------------------------------------------\n",
      "PRECISION = 0.99787,  RECALL = 0.99379, F_MESURE = 0.996 METHODE = probabiliste_lissé\n",
      "--------------------------------------------------------------------------------\n",
      "PRECISION = 0.99787,  RECALL = 0.99379, F_MESURE = 0.996 METHODE = Lucene\n",
      "--------------------------------------------------------------------------------\n",
      "PRECISION = 0.99833,  RECALL = 0.99523, F_MESURE = 0.997 METHODE = count\n",
      "--------------------------------------------------------------------------------\n",
      "PRECISION = 0.99598,  RECALL = 0.99429, F_MESURE = 0.995 METHODE = inverse_tfidf\n"
     ]
    }
   ],
   "source": [
    "for méthode in METHODE:\n",
    "    test_with_precision_recall(DICT, méthode, train =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1388888888888889, 0.1388888888888889)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer ='cette voiture nous semble immobilisé'\n",
    "true_answer = 'cette voiture nous semble immobilisé'\n",
    "precision_recall(answer, true_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'cette' in true_answer.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def metrics(lstcomp, lstref) :\n",
    "    card_intersec = 0.0 # force à utiliser la division non entière\n",
    "    for t in set(lstcomp) :\n",
    "        card_intersec += min(lstref.count(t), lstcomp.count(t))\n",
    "    precision = card_intersec/len(lstcomp)\n",
    "    rappel = card_intersec/len(lstref)\n",
    "    return (precision, rappel)\n",
    "metrics(answer, true_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "ACCURACY= 0.9863620981387479\n",
      "--------------------------------------------------------------------------------\n",
      "ACCURACY= 0.9863620981387479\n",
      "--------------------------------------------------------------------------------\n",
      "ACCURACY= 0.9863620981387479\n",
      "--------------------------------------------------------------------------------\n",
      "ACCURACY= 0.9863620981387479\n",
      "--------------------------------------------------------------------------------\n",
      "ACCURACY= 0.9863620981387479\n",
      "--------------------------------------------------------------------------------\n",
      "ACCURACY= 0.9883925549915398\n",
      "--------------------------------------------------------------------------------\n",
      "ACCURACY= 0.9828631858834904\n"
     ]
    }
   ],
   "source": [
    "for méthode in METHODE:\n",
    "    test_without_print(DICT, méthode, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "ACCURACY= 0.4775205455197845\n",
      "--------------------------------------------------------------------------------\n",
      "ACCURACY= 0.4775205455197845\n",
      "--------------------------------------------------------------------------------\n",
      "ACCURACY= 0.4727128532120922\n",
      "--------------------------------------------------------------------------------\n",
      "ACCURACY= 0.4727128532120922\n",
      "--------------------------------------------------------------------------------\n",
      "ACCURACY= 0.47571570784538164\n",
      "--------------------------------------------------------------------------------\n",
      "ACCURACY= 0.4877859719125184\n",
      "--------------------------------------------------------------------------------\n",
      "ACCURACY= 0.49010464567082057\n"
     ]
    }
   ],
   "source": [
    "for méthode in METHODE:\n",
    "    test_without_print(DICT, méthode, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "PRECISION = 0.99391,  RECALL = 0.99289, F_MESURE = 0.993 METHODE = normal\n",
      "--------------------------------------------------------------------------------\n",
      "PRECISION = 0.99391,  RECALL = 0.99289, F_MESURE = 0.993 METHODE = probabiliste\n",
      "--------------------------------------------------------------------------------\n",
      "PRECISION = 0.99391,  RECALL = 0.99289, F_MESURE = 0.993 METHODE = lissé\n",
      "--------------------------------------------------------------------------------\n",
      "PRECISION = 0.99391,  RECALL = 0.99289, F_MESURE = 0.993 METHODE = probabiliste_lissé\n",
      "--------------------------------------------------------------------------------\n",
      "PRECISION = 0.99391,  RECALL = 0.99289, F_MESURE = 0.993 METHODE = Lucene\n",
      "--------------------------------------------------------------------------------\n",
      "PRECISION = 0.99492,  RECALL = 0.99289, F_MESURE = 0.994 METHODE = count\n",
      "--------------------------------------------------------------------------------\n",
      "PRECISION = 0.99188,  RECALL = 0.98985, F_MESURE = 0.991 METHODE = inverse_tfidf\n"
     ]
    }
   ],
   "source": [
    "for méthode in METHODE:\n",
    "    test_with_precision_recall(DICT, méthode, train = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "PRECISION = 0.79171,  RECALL = 0.75531, F_MESURE = 0.773 METHODE = normal\n",
      "--------------------------------------------------------------------------------\n",
      "PRECISION = 0.79190,  RECALL = 0.75522, F_MESURE = 0.773 METHODE = probabiliste\n",
      "--------------------------------------------------------------------------------\n",
      "PRECISION = 0.79183,  RECALL = 0.74683, F_MESURE = 0.769 METHODE = lissé\n",
      "--------------------------------------------------------------------------------\n",
      "PRECISION = 0.79183,  RECALL = 0.74683, F_MESURE = 0.769 METHODE = probabiliste_lissé\n",
      "--------------------------------------------------------------------------------\n",
      "PRECISION = 0.79215,  RECALL = 0.75152, F_MESURE = 0.771 METHODE = Lucene\n",
      "--------------------------------------------------------------------------------\n",
      "PRECISION = 0.78681,  RECALL = 0.77215, F_MESURE = 0.779 METHODE = count\n",
      "--------------------------------------------------------------------------------\n",
      "PRECISION = 0.78363,  RECALL = 0.77918, F_MESURE = 0.781 METHODE = inverse_tfidf\n"
     ]
    }
   ],
   "source": [
    "for méthode in METHODE:\n",
    "    test_with_precision_recall(DICT, méthode, train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour, C'est le bot d'Avicen, posez vos questions, s'il vous plaît!\n",
      "Vous: bonjour\n",
      "bot:  bonjour\n",
      "Vous: où est la voiture\n",
      "bot:  la facture est disponible dans votre espace client\n",
      "Vous: où est ma voiture\n",
      "bot:  elle est à #position#\n",
      "Vous: combien de km mon véhicule a roulé\n",
      "bot:  il a parcouru #km#\n",
      "Vous: véhicule vitesse le plus élevé\n",
      "bot:  véhicule #id# #vitesse# #MAX#\n",
      "Vous: véhicule qui a un problème\n",
      "bot:  nous prenons en compte cet incident quel est le boitier concerné\n",
      "Vous: problème véhicule\n",
      "bot:  véhicule #id# #problème# #vitesse#\n",
      "Vous: \n"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour, C'est le bot d'Avicen, posez vos questions, s'il vous plaît!\n",
      "Vous: bonjour\n",
      "bot:  bonjour\n",
      "Vous: où est la voiture\n",
      "bot:  la voiture a été localisé situé ici #position#\n",
      "Vous: où est ma voiture\n",
      "bot:  elle est à #position#\n",
      "Vous: véhicule qui a un problème\n",
      "bot:  véhicule #id# #problème# #vitesse#\n",
      "Vous: voiture qui a un problème\n",
      "bot:  véhicule #id# #problème# #km#\n",
      "Vous: des problèmes de voitures\n",
      "bot:  il y a #nb_véhicules#\n",
      "Vous: problèmes de voitures\n",
      "bot:  il y a #nb_véhicules#\n",
      "Vous: \n"
     ]
    }
   ],
   "source": [
    "chat('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour, C'est le bot d'Avicen, posez vos questions, s'il vous plaît!\n",
      "Vous: bonjour\n",
      "bot:  bonjour\n",
      "Vous: bonsoir\n",
      "bot:  #oui/non#\n",
      "Vous: bonne nuit\n",
      "bot:  #oui/non#\n",
      "Vous: combien de voiture en route\n",
      "bot:  il y a #nb_veh#\n",
      "Vous: combien de voiture à l'arrêt\n",
      "bot:  il y a #nb_veh#\n",
      "Vous: voiture de vitesse maximale\n",
      "bot:  véhicule #id# #vitesse# #max#\n",
      "Vous: problème\n",
      "bot:  #oui/non#\n",
      "Vous: combien de voitures qui sont en circulation\n",
      "bot:  #total_veh_movement#\n",
      "Vous: \n"
     ]
    }
   ],
   "source": [
    "chat('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour, C'est le bot d'Avicen, posez vos questions, s'il vous plaît!\n",
      "Vous: bonjour\n",
      "bot:  bonjour\n",
      "Vous: voiture de vitesse minimale\n",
      "bot:  véhicule #id# #vitesse# #min#\n",
      "Vous: il y a des problèmes\n",
      "bot:  #oui/non#\n",
      "Vous: il y a des voitures à l'arrêt\n",
      "bot:  je vérifie cela tout de suite\n",
      "Vous: combien de voiture en circulations\n",
      "bot:  le valeur de rachat de ce véhicule a été définie lors de l'achat de celui-ci voir contrat\n",
      "Vous: combien de voiture à l'arrêt\n",
      "bot:  le valeur de rachat de ce véhicule a été définie lors de l'achat de celui-ci voir contrat\n",
      "Vous: \n"
     ]
    }
   ],
   "source": [
    "chat('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#oui/non#',\n",
       " '#nb_véhicules#',\n",
       " '#date#',\n",
       " '#adresse#',\n",
       " '#tel_comptabilite#',\n",
       " '#vin#',\n",
       " '#kpi_pollution#',\n",
       " '#kpi_finance#',\n",
       " '#kpi_gestion#',\n",
       " '#numéro_firmware#',\n",
       " '#co2#',\n",
       " '#roi#',\n",
       " '#code_dtc#',\n",
       " '#priorité#',\n",
       " '#transmission#vin#',\n",
       " '#énergie#',\n",
       " '#cv#',\n",
       " '#consumption#',\n",
       " '#date_fin_contrat#',\n",
       " '#date_last_entretien#',\n",
       " '#trajet#',\n",
       " '#état_traffic#',\n",
       " '#geoloc#',\n",
       " '#date_debut_contrat#',\n",
       " '#km#',\n",
       " '#total_veh_movement#',\n",
       " '#nb_module#',\n",
       " '#lab_program#',\n",
       " '#num_telephone#',\n",
       " '#val_trust_low#',\n",
       " '#val_trust_high#',\n",
       " '#listing_vin#',\n",
       " '#vin_proxi#',\n",
       " '#nb_place#',\n",
       " '#lastname#',\n",
       " '#user#',\n",
       " '#list_module#',\n",
       " '#detail_module#',\n",
       " '#tx_dispo#',\n",
       " '#calcul_roadtrip#',\n",
       " '#map_meteo#',\n",
       " '#brand_tyre#',\n",
       " '#spec_tyre#',\n",
       " '#tpms#',\n",
       " '#list_vin_free#',\n",
       " '#nb_vehicule#',\n",
       " '#list_vin_immediat#',\n",
       " '#list_vin_morning#',\n",
       " '#list_vin_afternoon#',\n",
       " '#list_vin_tonight#',\n",
       " '#list_alert#',\n",
       " '#list_podium#',\n",
       " '#nb_instances#',\n",
       " '#list_instances#',\n",
       " '#histo_trip#',\n",
       " '#nb_passagers#',\n",
       " '#vin_seat#',\n",
       " '#check_batterie#',\n",
       " '#list_vin_tomorrow#',\n",
       " '#list_cvs#',\n",
       " '#list_disconnect#',\n",
       " '#date_last_ct#',\n",
       " '#nb_amende#',\n",
       " '#last_vidange#',\n",
       " '#tco_vehicule#',\n",
       " '#tco_fleet#',\n",
       " '#tco_driver#',\n",
       " '#voltage#',\n",
       " '#list_garage#',\n",
       " '#list_lld#',\n",
       " '#vin_loyer#',\n",
       " '#contravention#',\n",
       " '#date_last_formation#',\n",
       " '#durée_contrat#',\n",
       " '#vin_veh#',\n",
       " '#wtpl#',\n",
       " '#rde#',\n",
       " '#contract#',\n",
       " '#position#',\n",
       " '#map_météo#',\n",
       " '#usure_plaquette_frein#',\n",
       " '#usure_pneumatique#',\n",
       " '#lave_glace#',\n",
       " '#consommation#',\n",
       " '#carburant_adapté#',\n",
       " '#km_moyenne#',\n",
       " '##total_veh_movement#',\n",
       " '#nb_véhicule#',\n",
       " '#ask_id#',\n",
       " '#nb_veh#',\n",
       " '#vitesse#',\n",
       " '#consumption_month#',\n",
       " '#horaire#',\n",
       " '#pression_pneu#',\n",
       " '#id#',\n",
       " '#max#',\n",
       " '#batterie#',\n",
       " '#min#',\n",
       " '#moyenne#',\n",
       " '#problème#',\n",
       " '#problèmes#',\n",
       " '#erreurs#',\n",
       " '#erreur#',\n",
       " '#danger#']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word in DICT['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PAIRS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e6f568ce17e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m         \u001b[0minput_lang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNameError\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_lang' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e6f568ce17e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNameError\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         \u001b[0mpairs1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_convos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m         \u001b[0minput_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs_trains\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs_tests\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepareData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPAIRS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'PAIRS' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import data\n",
    "import config\n",
    "import time\n",
    "from dateparser.search import search_dates\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "hidden_size = 20\n",
    "SOS_token  = config.SOS_token\n",
    "EOS_token  = config.EOS_token\n",
    "MAX_LENGTH = config.MAX_LENGTH\n",
    "stopwords  = config.STOPWORDS\n",
    "learning_rate = config.LEARNING_RATE\n",
    "teacher_forcing_ratio = 0.2\n",
    "dropout = config.DROPOUT\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  \n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "def normalizeString(s):\n",
    "    \"\"\"\n",
    "    Whith a tring s, we make it in lower case, delete \\n if exists at \n",
    "    the end of string, and delete specical case ? . and !\n",
    "    \"\"\"\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"([,.!?\\n])\", r\"\", s)# sumprimer tous les caractères .! et ?\n",
    "    #s = re.sub(r\"[^a-zA-Z0-9.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def readLangs():\n",
    "    print(\"Reading lines...\")\n",
    "    pairs_trains, pairs_tests = [], []   \n",
    "    encode_train = open(os.path.join(config.PROCESSED_PATH, \"question_train.txt\"), 'r')\n",
    "    decode_train = open(os.path.join(config.PROCESSED_PATH, \"answer_train.txt\"), 'r')\n",
    "    encode, decode = encode_train.readline(), decode_train.readline()\n",
    "    while encode and decode:\n",
    "        encode, decode = normalizeString(encode), normalizeString(decode)\n",
    "        decode = 'SOS '+ decode + ' EOS'\n",
    "        pairs_trains.append([encode, decode])\n",
    "        encode, decode = encode_train.readline(), decode_train.readline()\n",
    "    encode_train.close()\n",
    "    decode_train.close()\n",
    "    encode_test = open(os.path.join(config.PROCESSED_PATH, \"question_test.txt\"), 'r')\n",
    "    decode_test = open(os.path.join(config.PROCESSED_PATH, \"answer_test.txt\"), 'r')\n",
    "    encode, decode = encode_test.readline(), decode_test.readline()\n",
    "    while encode and decode:\n",
    "        encode, decode = normalizeString(encode), normalizeString(decode)\n",
    "        decode = 'SOS '+ decode + ' EOS'\n",
    "        pairs_tests.append([encode, decode])\n",
    "        encode, decode = encode_test.readline(), decode_test.readline()\n",
    "    encode_test.close()\n",
    "    decode_test.close()\n",
    "    return pairs_trains, pairs_tests\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH \n",
    "\n",
    "    \n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        for i in range(self.n_layers):\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        for i in range(self.n_layers):\n",
    "            output = F.relu(output)\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n",
    "        \n",
    "def closetWord(word, lang):\n",
    "    \"\"\"\n",
    "    find and return the closest word in lang\n",
    "    \"\"\"\n",
    "    Dict = lang.word2index\n",
    "    corpus = lang.index2word\n",
    "    if word in Dict:\n",
    "        return word\n",
    "    else:\n",
    "        distance = levenshtein(word, corpus[0])\n",
    "        close_word = corpus[0]\n",
    "        for ix in corpus:\n",
    "            if levenshtein(word, corpus[ix]) <distance:\n",
    "                close_word = corpus[ix]\n",
    "                distance = levenshtein(word, corpus[ix])\n",
    "        if distance <=1:\n",
    "            return close_word\n",
    "        else:\n",
    "            return word\n",
    "        \n",
    "def normalizeSentenceInChat(sentence):\n",
    "    sentence = sentence.strip().lower().split()\n",
    "    s = [closetWord(word, input_lang) for word in sentence]\n",
    "    return ' '.join(s)\n",
    "                    \n",
    "\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    #sentence = normalizeSentence\n",
    "    return [lang.word2index[word] for word in sentence.split(' ') if word in lang.word2index ]\n",
    "\n",
    "\n",
    "def variableFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    result = Variable(torch.LongTensor(indexes).view(-1, 1))\n",
    "    if use_cuda:\n",
    "        return result.cuda()\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "\n",
    "def variablesFromPair(pair):\n",
    "    input_variable = variableFromSentence(input_lang, pair[0])\n",
    "    target_variable = variableFromSentence(output_lang, pair[1])\n",
    "    return (input_variable, target_variable)\n",
    "\n",
    "\n",
    "def train(input_variable, target_variable, encoder, decoder, \n",
    "          encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_variable.size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "\n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_variable[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0][0]\n",
    "\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]]))\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "            decoder_input = target_variable[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            ni = topi[0][0]\n",
    "\n",
    "            decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "            if ni == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / target_length\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "def get_skip_step(n_iters):\n",
    "    return int(n_iters/10)\n",
    "\n",
    "def trainIters(n_iters, plot_every=100, learning_rate=learning_rate):\n",
    "    training_pairs = get_all_convos()\n",
    "    encoder, decoder = restore_model()\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "    skip_step = get_skip_step(n_iters)\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [variablesFromPair(random.choice(pairs_trains))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_variable = training_pair[0]\n",
    "        target_variable = training_pair[1]\n",
    "\n",
    "        loss = train(input_variable, target_variable, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % skip_step == 0:\n",
    "            print_loss_avg = print_loss_total / skip_step\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)\n",
    "    save_model(encoder, decoder)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    #sentence = normalizeSentenceInChat(sentence)\n",
    "    input_variable = variableFromSentence(input_lang, sentence)\n",
    "    input_length = input_variable.size()[0]\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_variable[ei],\n",
    "                                                 encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_outputs[ei] + encoder_output[0][0]\n",
    "\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]]))  \n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_hidden = decoder(\n",
    "            decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        ni = int(ni)\n",
    "        if ni == EOS_token:\n",
    "            #decoded_words.append('<EOS>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(output_lang.index2word[ni])\n",
    "\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def evaluate_randomly(n_iters=200, test=True):\n",
    "    encoder, decoder = restore_model()\n",
    "    if not test:\n",
    "        print('TEST error of word to word on TRAIN')\n",
    "        list_print_random = random.sample(range(n_iters),20 )\n",
    "        total_loss = 0\n",
    "        for i in range(n_iters):\n",
    "            pair = random.choice(pairs_trains)\n",
    "            output_words = evaluate(encoder, decoder, pair[0])\n",
    "            if 'EOS' in output_words:\n",
    "                l = output_words.index('EOS')+1\n",
    "            else:\n",
    "                l = len(output_words)+1\n",
    "            reponse = ' '.join(output_words[:l])\n",
    "            loss = _evaluate_by_right_word(pair[1], reponse)\n",
    "            total_loss +=loss\n",
    "            if i in list_print_random:\n",
    "                #answer = answers_with_data(pair[0], reponse)\n",
    "                print('Question: ', pair[0])\n",
    "                print('Réponse: ', pair[1])\n",
    "                print('Bot: {}   ACCURACY {:.1f}'.format(reponse, 1-loss))\n",
    "                #if answer != reponse:\n",
    "                 #   print('Réponse avec data:', answer)\n",
    "                \n",
    "                print('-'*50)\n",
    "        print('Accuracy of good answer word to word: ', 1-total_loss/n_iters)\n",
    "    else:\n",
    "        n_iters = len(pairs_tests)\n",
    "        total_loss = 0\n",
    "        random_print_index = random.sample(range(n_iters), 20)\n",
    "        for i in range(n_iters):\n",
    "            pair = pairs_tests[i]\n",
    "            output_words= evaluate(encoder, decoder, pair[0])\n",
    "            if 'EOS' in output_words:\n",
    "                l = output_words.index('EOS')+1\n",
    "            else:\n",
    "                l = len(output_words)+1\n",
    "            reponse = ' '.join(output_words[:l])\n",
    "            loss = _evaluate_by_right_word(pair[1], reponse)\n",
    "            if i in random_print_index:\n",
    "                print('Question: ', pair[0])\n",
    "                print('Réponse: ', pair[1])\n",
    "                print('Bot: {}. ACCURACY {:.1f}'.format(reponse, 1-loss))\n",
    "                print('-'*50)\n",
    "            total_loss +=loss\n",
    "        print('Test on {}'.format(n_iters))\n",
    "        print('Accuracy by percent of true words {}'.format(1-total_loss/n_iters))\n",
    " \n",
    "\n",
    "KEYWORD_TO_FUNCTION_DICT = {'#id#':'ID Véhicule','#km#':'Distance', '#check_batterie#':'Tension Batterie',\\\n",
    "                            '#vitesse#':'Vitesse Moyenne'}\n",
    "\n",
    "def answers_with_data(question, answer):\n",
    "    data = analyse_reponse(question, answer)\n",
    "    if data is None:\n",
    "        return answer\n",
    "    else:\n",
    "        index = 0\n",
    "        for word in answer.split():\n",
    "            if \"#\" in word:\n",
    "                try:\n",
    "                    good_answer = re.sub(word, str(data[index]), answer)\n",
    "                except IndexError:\n",
    "                    break\n",
    "                index +=1\n",
    "        return good_answer\n",
    "    \n",
    "                \n",
    "def analyse_reponse(question, answer):\n",
    "    \"\"\"\n",
    "    arg: question is the question of user, reponse is given by chatbot\n",
    "\n",
    "    \"\"\"\n",
    "    if '#' not in answer:\n",
    "        return None\n",
    "    else:\n",
    "        parameter = []\n",
    "        for key in KEYWORD_TO_FUNCTION_DICT:\n",
    "            if key in answer.split():\n",
    "                parameter.append(key)\n",
    "        times = find_right_time(question)\n",
    "        variables = _get_variable(answer)\n",
    "        data = []\n",
    "        if len(variables)==0:\n",
    "            return None\n",
    "        else:\n",
    "            ids = find_ids(question)\n",
    "            for variable in variables:\n",
    "                data.append(_get_data_from_table(ids, variable, times))\n",
    "            return data\n",
    "            \n",
    "                \n",
    "def find_right_time(question):\n",
    "    \"\"\"\n",
    "    cette fonction retourne période de temps dans la question\"\"\"\n",
    "    dictionnary = {'cette':'1', 'ce':'1', \"dernier\": '1',\"dernière\": '1' }\n",
    "    for word in dictionnary:\n",
    "        question = re.sub(word, dictionnary[word], question)\n",
    "    end_time = time.time()\n",
    "    try:\n",
    "         t = search_dates(question)\n",
    "    except ZeroDivisionError:\n",
    "        return None, None\n",
    "    if t == None:\n",
    "        return None, None\n",
    "    \n",
    "    else:\n",
    "        if len(t)>=2:\n",
    "        \n",
    "            t0 = t[0][-1]\n",
    "            start_time = time.mktime(t0.timetuple())\n",
    "                \n",
    "            t1 = t[-1][-1]\n",
    "            end_time = time.mktime(t1.timetuple())\n",
    "        if len(t)==1:\n",
    "            t = t[0][1]\n",
    "            start_time = time.mktime(t.timetuple())\n",
    "    #start_time, end_time = find_good_time_in_table(start_time, end_time)\n",
    "    return start_time, end_time\n",
    "    \n",
    "\n",
    "def _get_variable(reponse):\n",
    "    list_variable =[]\n",
    "    for word in reponse.split():\n",
    "        if \"#\" in word:\n",
    "            list_variable.append(word)\n",
    "    #if variable in KEYWORD_TO_FUNCTION_DICT:\n",
    "    return [KEYWORD_TO_FUNCTION_DICT[variable] for variable in list_variable\n",
    "               if variable in KEYWORD_TO_FUNCTION_DICT]\n",
    "\n",
    "    \n",
    "def _get_data_from_table(ids, variable, time):\n",
    "    \"\"\"\n",
    "    temps est un couple de début et fin\n",
    "    \"\"\"\n",
    "    df = pd.read_csv('fleet_donnees.csv', sep = ';')\n",
    "    #time1 = string_to_datetime(time[0])\n",
    "    #time2 = string_to_datetime(time[1])\n",
    "    if variable is None:\n",
    "        return \"#TO_CONSTRUCT#\"\n",
    "    if variable =='Tension Batterie':\n",
    "        return min(df['Tension Batterie'])\n",
    "    time1 = time[0]\n",
    "    time2 = time[1]\n",
    "    if time1 is None:\n",
    "        time1 = string_to_datetime(df.iloc[0]['Debut période'])\n",
    "    if time2 is None:\n",
    "        time2 = string_to_datetime(df.iloc[len(df)-1]['Fin Période'])\n",
    "    if ids is not None:\n",
    "        try:\n",
    "            df = df.loc[(df['ID Véhicule'] ==  int(ids))] \n",
    "        except ValueError:\n",
    "            pass\n",
    "    List = []\n",
    "    for i in range(len(df)):\n",
    "        if string_to_datetime(df.iloc[i]['Debut période']) >=time1 and \\\n",
    "           string_to_datetime(df.iloc[i]['Fin Période'])<=time2:\n",
    "                List.append(i)\n",
    "\n",
    "    list_data_to_return = df.iloc[List][variable]\n",
    "    if len(list_data_to_return)>1:\n",
    "        t = sum(list_string_to_float(list_data_to_return))/len(list_data_to_return)\n",
    "    else:\n",
    "        t= list_data_to_return\n",
    "    \n",
    "   # try:\n",
    "       # t=sum(list_data_to_return.values[0])/len(list_to_return.values[0])\n",
    "   # except AttributeError or IndexError:\n",
    "   # t = sum(list_data_to_return)/len(list_data_to_return)\n",
    "    return t\n",
    "        \n",
    "          \n",
    "def find_ids(question):\n",
    "    if 'tous' in question or 'toutes' in question or 'tout'in question or 'toute' in question:\n",
    "        return None\n",
    "    return None\n",
    "    #else:\n",
    "     #   print('La liste de ID des voitures:')\n",
    "      #  imprimer_id()\n",
    "       # return str(input('indiquez l\\'identifiant de ce véhicule:'))\n",
    "    \n",
    "\n",
    "def string_to_datetime(string):\n",
    "    t = search_dates(string)\n",
    "    t = t[0][1]\n",
    "    t = time.mktime(t.timetuple())\n",
    "    return t\n",
    "\n",
    "def list_string_to_float(List_string):\n",
    "    List = []\n",
    "    for string in List_string:\n",
    "        if type(string)==str:\n",
    "            string = re.sub(',', '.', string)\n",
    "        else:\n",
    "            pass\n",
    "        string2float  = float(string)\n",
    "        List.append(string2float)\n",
    "    return List\n",
    "    \n",
    "def find_good_time_in_table(begin_time, end_time):\n",
    "    \"\"\"\n",
    "    begin_time and end_time is in float type\n",
    "    return right begin_time and end_time in table\"\"\"   \n",
    "    import pandas as pd\n",
    "    df = pd.read_csv('fleet_donnees.csv', sep = ';')\n",
    "    start_time_colone = df['Debut période']\n",
    "    end_time_colone = df['Fin Période']\n",
    "    start_time_table = []\n",
    "    for i in start_time_colone:\n",
    "        time,_=find_right_time(i)\n",
    "        start_time_table.append(time)\n",
    "    end_time_table = []\n",
    "    for i in fin_time_colone:\n",
    "        time, _ = find_right_time(i)\n",
    "        end_time_table.append(time)\n",
    "    start = start_time_table[-1]\n",
    "    for i in start_time_table:\n",
    "        if begin_time <= i:\n",
    "            start = i\n",
    "            break\n",
    "    end = end_time_table[-1]\n",
    "    for i in end_time_table:\n",
    "        if end <=i:\n",
    "            end = i\n",
    "            break\n",
    "    #start = start_time_table[start_time_table==start].index[0]\n",
    "    #end = fin_time_table[fin_time_table==end].index[0]\n",
    "    start = start_time_table.index(start)\n",
    "    end =  end_time_table.index(end)\n",
    "    start = start_time_colone[start]\n",
    "    end = _time_colone[end]\n",
    "    return start, end    \n",
    "    \n",
    "    \n",
    "def _evaluate_by_right_word(reponse, bonne_reponse):\n",
    "    reponse = reponse.split()\n",
    "    bonne_reponse = bonne_reponse.split()\n",
    "    min_length = min(len(reponse), len(bonne_reponse))\n",
    "    max_length = max(len(reponse), len(bonne_reponse))\n",
    "    error = max_length-min_length\n",
    "    for i in range(min_length):\n",
    "        if reponse[i] != bonne_reponse[i]:\n",
    "            error +=1\n",
    "    return error/max_length      \n",
    "        \n",
    "def make_dir(path):\n",
    "    \"\"\" Create a directory if there isn't one already. \"\"\"\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        pass\n",
    "def save_model(encoder, decoder):        \n",
    "    make_dir(config.CHECK_POINT_PATH)\n",
    "    path1= os.path.join(config.CHECK_POINT_PATH, 'encoder_simulation_2.ck')\n",
    "    path2 = os.path.join(config.CHECK_POINT_PATH, 'decoder_simulation_2.ck')\n",
    "    try: \n",
    "        os.remove(path1)  \n",
    "        os.remove(path2)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    torch.save(encoder,path1)\n",
    "    torch.save(decoder, path2)\n",
    "    \n",
    "memo = {}\n",
    "def levenshtein(s, t):\n",
    "    \"\"\"\n",
    "    Pour calculer la distance  Levenshtein entre 2 string s et t\n",
    "    \"\"\"\n",
    "    if s == \"\":\n",
    "        return len(t)\n",
    "    if t == \"\":\n",
    "        return len(s)\n",
    "    cost = 0 if s[-1] == t[-1] else 1\n",
    "       \n",
    "    i1 = (s[:-1], t)\n",
    "    if not i1 in memo:\n",
    "        memo[i1] = levenshtein(*i1)\n",
    "    i2 = (s, t[:-1])\n",
    "    if not i2 in memo:\n",
    "        memo[i2] = levenshtein(*i2)\n",
    "    i3 = (s[:-1], t[:-1])\n",
    "    if not i3 in memo:\n",
    "        memo[i3] = levenshtein(*i3)\n",
    "    res = min([memo[i1]+1, memo[i2]+1, memo[i3]+cost])\n",
    "    \n",
    "    return res\n",
    "    \n",
    "def restore_model():\n",
    "    #hidden_size = hidden_size\n",
    "    path1 = os.path.join(config.CHECK_POINT_PATH, 'encoder_simulation_2.ck')\n",
    "    path2 = os.path.join(config.CHECK_POINT_PATH, 'decoder_simulation_2.ck')\n",
    "    if os.path.exists(path1)and os.path.exists(path2):\n",
    "        print('Reading the parameters from {} and {}...'.format(path1, path2))\n",
    "        encoder = torch.load(path1)\n",
    "        decoder = torch.load(path2)\n",
    "    else:\n",
    "        print('Initializing fresh parameters...')\n",
    "        encoder = EncoderRNN(input_lang.n_words, hidden_size)\n",
    "        decoder = DecoderRNN(hidden_size, output_lang.n_words,1)\n",
    "\n",
    "    if use_cuda:\n",
    "        encoder = encoder.cuda()\n",
    "        decoder = decoder.cuda()\n",
    "    return encoder, decoder \n",
    "\n",
    "def imprimer_id():\n",
    "    df = pd.read_csv('fleet_donnees.csv', sep = ';')   \n",
    "    df = df['ID Véhicule']\n",
    "    List_voiture = []\n",
    "    for i in df:\n",
    "        if i not in List_voiture:\n",
    "            List_voiture.append(i)\n",
    "    for i in List_voiture:\n",
    "        print(i, end='  ')\n",
    "\n",
    "def construct_dict(lang):\n",
    "    in_file = open(os.path.join(config.PROCESSED_PATH, 'dictionnary.txt'), 'r')\n",
    "    lines = []\n",
    "    dicts = lang.word2index.copy()\n",
    "    for line in in_file:\n",
    "        line = line.strip().split('|')\n",
    "        for word in dicts:\n",
    "            if str(word) in line and len(word)>0:\n",
    "                line.insert(0,str(word))\n",
    "                lines.append(line)\n",
    "                del dicts[str(word)]\n",
    "                break\n",
    "    return lines\n",
    "\n",
    "def find_close_line(lines, lang, line):\n",
    "    LINE = []\n",
    "    for word in line.strip().split():\n",
    "        if word in lang.word2index:\n",
    "            LINE.append(word)\n",
    "        else:\n",
    "            for pharse in lines:\n",
    "                if word in pharse:\n",
    "                    LINE.append(pharse[0])\n",
    "                    break\n",
    "    return ' '.join(LINE)\n",
    "\n",
    "def chat():\n",
    "    encoder, decoder = restore_model()\n",
    "    make_dir(config.CHECK_POINT_PATH)\n",
    "    lines = construct_dict(input_lang)\n",
    "    output_file = open(os.path.join(config.CHECK_POINT_PATH, 'convos70.txt'), 'a+')\n",
    "    print('Bonjour, c\\'est le Bot d\\'AVICEN, Je peux vous aider? \\n')\n",
    "    while True:\n",
    "            line = str(input('Vous: '))\n",
    "            if len(line) > 0 and line[-1] == '\\n':\n",
    "                line = line[:-1]\n",
    "            if line == '':\n",
    "                break\n",
    "            line = normalizeString(line)\n",
    "            #transform_line  = normalizeSentenceInChat(line)\n",
    "            #transform_line = find_close_line(lines, input_lang, transform_line)\n",
    "           # print('LIGNE TRANFORMÉ: ', line)\n",
    "            output_file.write('VOUS ++++ ' + line + '\\n')\n",
    "            reponse = evaluate(encoder, decoder, line)\n",
    "            if 'SOS' in reponse:\n",
    "                 reponse = reponse[1:] \n",
    "            if 'EOS' in reponse:\n",
    "                reponse = reponse[:-1] \n",
    "            reponse = \" \".join(reponse)\n",
    "            output_file.write('BOT ++++ ' + reponse + '\\n')\n",
    "           # print('Without Data: ', reponse)\n",
    "            #reponse = answers_with_data(line, reponse)\n",
    "            print('Bot AVICEN: ', reponse)\n",
    "            print('-'*50)\n",
    "    output_file.close() \n",
    "    \n",
    "\n",
    "def test(pairs_test):\n",
    "    \"\"\"\n",
    "    Use test for know how our model is good\n",
    "    \"\"\"\n",
    "    encoder, decoder = restore_model()\n",
    "    total_loss = 0\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "    for pair in pairs_test:\n",
    "        variable = variablesFromPair(pair)\n",
    "        loss = train( variable[0], variable[1], encoder, decoder, encoder_optimizer, decoder_optimizer,\\\n",
    "                     criterion= criterion)\n",
    "        total_loss +=loss\n",
    "    Length_inputs = len(pairs_test) if len(pairs_test) !=0 else 1\n",
    "    return total_loss/Length_inputs\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################-----------------------\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    try:\n",
    "        input_lang\n",
    "    except NameError :\n",
    "        pairs1 = get_all_convos()\n",
    "        input_lang, output_lang, pairs_trains, pairs_tests = prepareData(pairs1, PAIRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Ridge Classifier\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
      "        max_iter=None, normalize=False, random_state=None, solver='lsqr',\n",
      "        tol=0.01)\n",
      "train time: 0.158s\n",
      "test time:  0.001s\n",
      "accuracy:   0.885\n",
      "================================================================================\n",
      "Perceptron\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,\n",
      "      max_iter=None, n_iter=50, n_jobs=1, penalty=None, random_state=0,\n",
      "      shuffle=True, tol=None, verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:319: UserWarning: In Ridge, only 'sag' solver can currently fit the intercept when X is sparse. Solver has been automatically changed into 'sag'.\n",
      "  warnings.warn(\"In Ridge, only 'sag' solver can currently fit the \"\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 0.153s\n",
      "test time:  0.001s\n",
      "accuracy:   0.963\n",
      "================================================================================\n",
      "Passive-Aggressive\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "PassiveAggressiveClassifier(C=1.0, average=False, class_weight=None,\n",
      "              fit_intercept=True, loss='hinge', max_iter=None, n_iter=50,\n",
      "              n_jobs=1, random_state=None, shuffle=True, tol=None,\n",
      "              verbose=0, warm_start=False)\n",
      "train time: 0.139s\n",
      "test time:  0.001s\n",
      "accuracy:   0.970\n",
      "================================================================================\n",
      "kNN\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=10, p=2,\n",
      "           weights='uniform')\n",
      "train time: 0.001s\n",
      "test time:  0.014s\n",
      "accuracy:   0.644\n",
      "================================================================================\n",
      "Random forest\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 0.258s\n",
      "test time:  0.035s\n",
      "accuracy:   0.986\n"
     ]
    }
   ],
   "source": [
    "STOPWORDS = ['de','d','le','la','l','du','d','ce','c','m','me','ma','si','t','sur'\\\n",
    "           'n','en','il', 'les','des','est','sont','s', 'a','y','au','un','une',\\\n",
    "          'on', 'nous', 'je', 'j','vous']\n",
    "\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "\n",
    "y_train = question_class\n",
    "ch2 = SelectKBest(chi2)\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=1.0, min_df =0.0)\n",
    "                                 #stop_words='english')\n",
    "X_train = vectorizer.fit_transform(QUESTION_TO_TRAINS)\n",
    "#X_train = ch2.fit_transform(X_train, y_train)\n",
    "\n",
    "\n",
    "X_test = QUESTION_TO_TRAINS\n",
    "X_test = vectorizer.fit_transform(X_test)\n",
    "#X_test = ch2.transform(X_test)\n",
    "results = []\n",
    "\n",
    "def calcul_score(x,y):\n",
    "    score = 0\n",
    "    for i in range(len(x)):\n",
    "        if x[i]==y[i]:\n",
    "            score+=1\n",
    "    return score/len(x)\n",
    "\n",
    "def benchmark(clf):\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time.time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = time.time()\n",
    "    pred = clf.predict(X_test)\n",
    "    #pred = pred[0]\n",
    "    #print(pairs_trains[pred])\n",
    "    test_time = time.time() - t0\n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "    score = calcul_score(y_train, pred)\n",
    "    print(\"accuracy:   %0.3f\" % score)\n",
    "for clf, name in (\n",
    "        (RidgeClassifier(tol=1e-2, solver=\"lsqr\"), \"Ridge Classifier\"),\n",
    "        (Perceptron(n_iter=50), \"Perceptron\"),\n",
    "        (PassiveAggressiveClassifier(n_iter=50), \"Passive-Aggressive\"),\n",
    "        (KNeighborsClassifier(n_neighbors=10), \"kNN\"),\n",
    "        (RandomForestClassifier(n_estimators=100), \"Random forest\")):\n",
    "    print('=' * 80)\n",
    "    print(name)\n",
    "    results.append(benchmark(clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "ACCURACY= 0.37597199928660596\n",
      "--------------------------------------------------------------------------------\n",
      "ACCURACY= 0.37597199928660596\n",
      "--------------------------------------------------------------------------------\n",
      "ACCURACY= 0.3750356696985909\n",
      "--------------------------------------------------------------------------------\n",
      "ACCURACY= 0.3750356696985909\n"
     ]
    }
   ],
   "source": [
    "for méthode in METHODE:\n",
    "    test_without_print(DICT, méthode, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_and_all_convos():\n",
    "    convos = []\n",
    "    file1 = 'chatbot_tout_corpus.txt'\n",
    "    file2 = 'chatbot_tout_corpus_13juin.txt'\n",
    "    file3 = 'convos_120_nettoye.txt' # cette fichier viens d'etre ajouter\n",
    "    #file4 = 'chatbot.txt'\n",
    "    #file5 = 'convosNettoye.txt'\n",
    "    #liste_file = [file1, file2, file3, file4, file5]\n",
    "    liste_file = [file1, file2, file3]\n",
    "    for file in liste_file:\n",
    "        with open(file) as f:\n",
    "            i=0\n",
    "            for line in f:\n",
    "                if i%2==0:\n",
    "                    question = clearing(line)\n",
    "                    if '++++' in question:\n",
    "                         question = question[9:]\n",
    "                else:\n",
    "                    answer = clearing(line)\n",
    "                    if '++++' in answer:\n",
    "                        answer = answer[9:]\n",
    "                    convos.append([question, answer])\n",
    "                i+=1\n",
    "        f.close()\n",
    "    convos2 = simulation_pairs()\n",
    "    convos.extend(convos2)\n",
    "    return convos\n",
    "\n",
    "def nettoye_all_convos(all_convos):\n",
    "    \"\"\"\n",
    "    Supprimer tous les couples où les questions sont les mêmes\"\"\"\n",
    "    index_of_convos = []\n",
    "    question = []\n",
    "    index =0\n",
    "    for pair in all_convos:\n",
    "        if pair[0] not in question:\n",
    "            index_of_convos.append(index)\n",
    "            question.append(pair[0])\n",
    "        index +=1\n",
    "    convos = [all_convos[index] for index in index_of_convos]\n",
    "    print('nous avons sumpprimer {} couples parmis {} couples'.format(len(all_convos)-len(index_of_convos),index))\n",
    "    return convos\n",
    "def save_convos_in_file(all_convos):\n",
    "    with open('convos27juin.txt', 'w') as f:\n",
    "        for pair in all_convos:\n",
    "            f.write('YOU ++++ '+ pair[0]+'\\n')\n",
    "            f.write('BOT ++++ '+ pair[1]+'\\n')\n",
    "    f.close()\n",
    "    \n",
    "def get_only_key_in_question():\n",
    "    convos = []\n",
    "    file1 = 'convos27.juin.txt'\n",
    "    liste_file = [file1]\n",
    "    for file in liste_file:\n",
    "        with open(file) as f:\n",
    "            i=0\n",
    "            for line in f:\n",
    "                if i%2==0:\n",
    "                    question = clearing(line)\n",
    "                    if '++++' in question:\n",
    "                         question = question[9:]\n",
    "                else:\n",
    "                    answer = clearing(line)\n",
    "                    if '++++' in answer:\n",
    "                        answer = answer[9:]\n",
    "                    convos.append([question, answer])\n",
    "                i+=1\n",
    "        f.close()\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1506"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_convos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nous avons sumpprimer 274 couples parmis 1506 couples\n"
     ]
    }
   ],
   "source": [
    "all_convos = nettoye_all_convos(all_convos)\n",
    "save_convos_in_file(all_convos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
