{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing raw data into train set ...\n",
      "Preparing data to be model-ready ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\"\"\" A neural chatbot using sequence to sequence model with\n",
    "attentional decoder. \n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "import config as config\n",
    "\n",
    "\n",
    "##############\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stop_word = ['?', '.']\n",
    "def clearing_word(word):\n",
    "    word = re.sub('\\x8e', 'é', word)\n",
    "    word = re.sub('\\x88', 'à', word)\n",
    "    word = re.sub('\\x9d', 'ù', word)\n",
    "    word = re.sub('\\x8f', 'è', word)\n",
    "    word = re.sub('\\x9e', 'û', word)\n",
    "    word = re.sub('\\x90', 'ê', word)\n",
    "    word = re.sub('\\x99', 'ô', word)\n",
    "    word = re.sub('\\x94', 'î', word)\n",
    "    word = re.sub('\\x8f', 'è', word)\n",
    "    return word\n",
    "    \n",
    "def clearing(pharse):\n",
    "    \"\"\"\n",
    "    Arg: Data is a list of questions or answers\n",
    "    Return: clean questions et answers \n",
    "    \"\"\"\n",
    "    #clean_data = []\n",
    "    pharses =[]\n",
    "    for word in pharse.split(' '):\n",
    "       # line= data[i].lower().split(' ')\n",
    "        \n",
    "        #for word in line:\n",
    "        word = clearing_word(word)\n",
    "        if word not in stop_word and len(word) !=0:\n",
    "                    \n",
    "            pharses.append(word)\n",
    "        \n",
    "    return ' '.join(pharses)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stop_word = ['?', '.']\n",
    "def clearing_word(word):\n",
    "    word = re.sub('\\x8e', 'é', word)\n",
    "    word = re.sub('\\x88', 'à', word)\n",
    "    word = re.sub('\\x9d', 'ù', word)\n",
    "    word = re.sub('\\x8f', 'è', word)\n",
    "    word = re.sub('\\x9e', 'û', word)\n",
    "    word = re.sub('\\x90', 'ê', word)\n",
    "    word = re.sub('\\x99', 'ô', word)\n",
    "    word = re.sub('\\x94', 'î', word)\n",
    "   # word = re.sub('\\x8f', 'è', word)\n",
    "    word = re.sub('\\x8d', 'ç', word)\n",
    "    word = re.sub('õ', '', word)\n",
    "    word = re.sub('Ê', '', word)\n",
    "    word = re.sub('[?,.,!, \\,,  %]', '', word)\n",
    "    if word == 'û' or word == 'v' or word == 'é':\n",
    "        word = ''\n",
    "    if word ==\"2017êles\":\n",
    "        word = \"2017\"\n",
    "    if \"ênox\" in word:\n",
    "        word =\"nox\"\n",
    "    return word\n",
    "    \n",
    "def clearing(pharse):\n",
    "    \"\"\"\n",
    "    Arg: Data is a list of questions or answers\n",
    "    Return: clean questions et answers \n",
    "    \"\"\"\n",
    "    #clean_data = []\n",
    "    pharses =[]\n",
    "    for word in pharse.strip().lower().split(' '):\n",
    "       # line= data[i].lower().split(' ')\n",
    "        \n",
    "        #for word in line:\n",
    "        word = clearing_word(word)\n",
    "        if word not in stop_word and len(word) !=0:       \n",
    "            pharses.append(word)\n",
    "        \n",
    "    return ' '.join(pharses)\n",
    "\n",
    "\n",
    "\n",
    "def get_all_convos():\n",
    "    convos = []\n",
    "    file1 = 'chatbot_tout_corpus.txt'\n",
    "    file2 = 'convos_120_nettoye.txt'\n",
    "    liste_file = [file1, file2]\n",
    "    for file in liste_file:\n",
    "        with open(file) as f:\n",
    "            i=0\n",
    "            for line in f:\n",
    "                if i%2==0:\n",
    "                    question = clearing(line)\n",
    "                    if '++++' in question:\n",
    "                         question = question[9:]\n",
    "                else:\n",
    "                    answer = clearing(line)\n",
    "                    if '++++' in answer:\n",
    "                        answer = answer[9:]\n",
    "                    convos.append([question, answer])\n",
    "                i+=1\n",
    "        f.close()\n",
    "    return convos\n",
    "\n",
    "def train_test():\n",
    "    convos = get_all_convos()\n",
    "    trains, tests = [], []\n",
    "    rang = len(convos)\n",
    "    index_of_train = random.sample(range(rang), int(0.8*rang))\n",
    "    for i in range(rang):\n",
    "        if i in index_of_train:\n",
    "            trains.append(convos[i])\n",
    "        else:\n",
    "            tests.append(convos[i])\n",
    "    save_train_test_in_file(trains, False)\n",
    "    save_train_test_in_file(tests, True)\n",
    "    \n",
    "def save_train_test_in_file(CONVOS, answer=True):\n",
    "    if not answer:\n",
    "        out_path1 = os.path.join(config.PROCESSED_PATH, 'train_question.txt')\n",
    "        out_path2 = os.path.join(config.PROCESSED_PATH, 'train_answer.txt')\n",
    "    else: \n",
    "        out_path1 = os.path.join(config.PROCESSED_PATH, 'test_question.txt')\n",
    "        out_path2 = os.path.join(config.PROCESSED_PATH, 'test_answer.txt')\n",
    "    question, answer = [], []\n",
    "    for i in range(len(CONVOS)):\n",
    "        question.append(CONVOS[i][0])\n",
    "        answer.append(CONVOS[i][1])\n",
    "    with open(out_path1, 'w') as f1:\n",
    "        for i in range(len(question)):\n",
    "            f1.write(question[i]+'\\n')\n",
    "    f1.close()\n",
    "    with open(out_path2, 'w') as f2:\n",
    "        for i in range(len(answer)):\n",
    "            f2.write(answer[i]+'\\n')\n",
    "    f2.close()\n",
    "\n",
    "def question_answers():\n",
    "    \"\"\" Divide the dataset into two sets: questions and answers. \"\"\"\n",
    "    convos = get_all_convos()\n",
    "    questions_train, answers_train = [], []\n",
    "    questions_test, answers_test = [], []\n",
    "    index_of_train = random.sample(range(len(convos)), int(0.8*len(convos)))\n",
    "    for i in range(len(convos)):\n",
    "        convo = convos[i]\n",
    "        question = [];answer = []\n",
    "        for word in convo[0].split():\n",
    "            if word not in config.STOPWORDS and word not in \"([.,!?\\\"'-<>:;)(])\":\n",
    "                question.append(word)\n",
    "        for word in convo[-1].split():\n",
    "            if word not in config.STOPWORDS and word not in \"([.,!?\\\"'-<>:;)(])\":\n",
    "                answer.append(word)\n",
    "        question = ' '.join(word for word in question)\n",
    "        answer = ' '.join(word for word in answer)\n",
    "        if i in index_of_train:\n",
    "            questions_train.append(question)\n",
    "            answers_train.append(answer)\n",
    "        else:\n",
    "            questions_test.append(question)\n",
    "            answers_test.append(answer)\n",
    "    return questions_train, answers_train, questions_test, answers_test\n",
    "\n",
    "\n",
    "        \n",
    "def prepare_dataset():\n",
    "    make_dir(config.PROCESSED_PATH)\n",
    "    path1= os.path.join(config.PROCESSED_PATH, 'question_train.txt')\n",
    "    path2 = os.path.join(config.PROCESSED_PATH, 'answer_train.txt')\n",
    "    try: \n",
    "        os.remove(path1);   os.remove(path2)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    with open(path1, 'w') as f:\n",
    "        for i in range(len(questions_train)):\n",
    "            f.write(questions_train[i]+'\\n')\n",
    "    f.close()\n",
    "    with open(path2, 'w') as f:\n",
    "        for i in range(len(answers_train)):\n",
    "            f.write(answers_train[i]+'\\n')\n",
    "    f.close()\n",
    "    path3= os.path.join(config.PROCESSED_PATH, 'question_test.txt')\n",
    "    path4 = os.path.join(config.PROCESSED_PATH, 'answer_test.txt')\n",
    "    try: \n",
    "        os.remove(path3);   os.remove(path4)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    with open(path3, 'w') as f:\n",
    "        for i in range(len(questions_test)):\n",
    "            f.write(questions_test[i]+'\\n')\n",
    "    f.close()\n",
    "    with open(path4, 'w') as f:\n",
    "        for i in range(len(answers_test)):\n",
    "            f.write(answers_test[i]+'\\n')\n",
    "    f.close()\n",
    "\n",
    "def make_dir(path):\n",
    "    \"\"\" Create a directory if there isn't one already. \"\"\"\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "\n",
    "def basic_tokenizer(lines, normalize_digits=True):\n",
    "    \"\"\" A basic tokenizer to tokenize text into tokens.\n",
    "    Feel free to change this to suit your need. \"\"\"\n",
    "    words = []\n",
    "    if type(lines) == str:\n",
    "        for word in lines.strip().lower().split(' '):\n",
    "                if not word:\n",
    "                      continue\n",
    "                words.append(word)\n",
    "    else:\n",
    "        for line in lines:\n",
    "            for word in line.strip().lower().split(' '):\n",
    "                    if not word:\n",
    "                        continue\n",
    "                    words.append(word)\n",
    "    return words\n",
    "\n",
    "\n",
    "def build_vocab():\n",
    "         \n",
    "    convos = get_all_convos()\n",
    "    out_path_dec = os.path.join(config.PROCESSED_PATH, 'decoder_vocab.txt')\n",
    "    out_path_enc = os.path.join(config.PROCESSED_PATH, 'encoder_vocab.txt')\n",
    "    try: \n",
    "        os.remove(out_path_dec) and os.remove(out_path_enc)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for i in range(len(convos)):\n",
    "        convo = convos[i]\n",
    "        for word in convo[0].strip().split():\n",
    "            if word not in config.STOPWORDS and word not in \"([.,!?\\\"'-<>:;)(])\":\n",
    "                if word not in questions:\n",
    "                    questions.append(word)\n",
    "        for word in convo[-1].split():\n",
    "            if word not in config.STOPWORDS and word not in \"([.,!?\\\"'-<>:;)(])\":\n",
    "                if word not in answers:\n",
    "                    answers.append(word)\n",
    "    with open(out_path_enc, 'w') as f:\n",
    "            f.write('<pad>' + '\\n')\n",
    "            f.write('<unk>' + '\\n')\n",
    "            f.write('<s>' + '\\n')\n",
    "            f.write('<\\s>' + '\\n') \n",
    "            for word in questions:\n",
    "                f.write(word + '\\n')\n",
    "    f.close()\n",
    "    with open(out_path_dec, 'w') as f:\n",
    "            f.write('<pad>' + '\\n')\n",
    "            f.write('<unk>' + '\\n')\n",
    "            f.write('<s>' + '\\n')\n",
    "            f.write('<\\s>' + '\\n') \n",
    "            for word in answers:\n",
    "                f.write(word + '\\n')\n",
    "    f.close()\n",
    "    enc, dec = len(questions), len(answers)\n",
    "    with open('config.py', 'a') as cf:\n",
    "                cf.write('DEC_VOCAB = '+str(dec)+'\\n')\n",
    "                cf.write('NUM_SAMPLES = '+str(dec-1)+'\\n')\n",
    "                cf.write('ENC_VOCAB = '+str(enc)+'\\n')\n",
    "    cf.close()\n",
    "\n",
    "def load_vocab(vocab_path):\n",
    "    with open(vocab_path, 'r') as f:\n",
    "        words = f.read().splitlines()\n",
    "    return words, {words[i]: i for i in range(len(words))}\n",
    "\n",
    "def sentence2id(vocab, line):\n",
    "    line = basic_tokenizer(line)\n",
    "    words = []\n",
    "    for word in line:\n",
    "        if word in vocab:\n",
    "            words.append(word)\n",
    "    return [vocab.get(token) for token in words]\n",
    "\n",
    "def token2id(data, answer):\n",
    "    \"\"\" Convert all the tokens in the data into their corresponding\n",
    "    index in the vocabulary. \"\"\"\n",
    "    if answer: \n",
    "         vocab_path = 'decoder_vocab.txt'\n",
    "    else:\n",
    "        vocab_path = 'encoder_vocab.txt'\n",
    "    out_path = data[:-4]+'2id.txt'\n",
    "    _, vocab = load_vocab(os.path.join(config.PROCESSED_PATH, vocab_path))\n",
    "    in_file = open(os.path.join(config.PROCESSED_PATH, data), 'r')\n",
    "    out_file = open(os.path.join(config.PROCESSED_PATH, out_path), 'w')\n",
    "    \n",
    "    lines = in_file.read().splitlines()\n",
    "    for line in lines:\n",
    "        if data == \"answer_test.txt\" or data == \"answer_train.txt\":\n",
    "            ids = [vocab['<s>']]\n",
    "        else:\n",
    "            ids = []\n",
    "        ids.extend(sentence2id(vocab, line))#vocab est un dict\n",
    "        if data == \"answer_test.txt\" or data == \"answer_train.txt\":\n",
    "            ids.append(vocab['<\\s>'])\n",
    "        out_file.write(' '.join(str(id_) for id_ in ids) + '\\n')\n",
    "\n",
    "def prepare_raw_data():\n",
    "    print('Preparing raw data into train set ...')\n",
    "    questions_train, answers_train, questions_test, answers_test = question_answers()\n",
    "    prepare_dataset()\n",
    "    #build_vocab(questions, False)\n",
    "    #build_vocab(answers, True)\n",
    "\n",
    "def process_data():\n",
    "    print('Preparing data to be model-ready ...')\n",
    "    token2id('question_train.txt', False)\n",
    "    token2id('answer_train.txt', True)\n",
    "    token2id('question_test.txt', False)\n",
    "    token2id('answer_test.txt', True)\n",
    "    print('Done!')\n",
    "\n",
    "def load_data(enc_filename, dec_filename, max_training_size=None):\n",
    "    \"\"\"\n",
    "    Arg: enc_filename is questions2id.txt file and dec_filename is answers2id.txt file\n",
    "    \"\"\"\n",
    "    encode_file = open(os.path.join(config.PROCESSED_PATH, enc_filename), 'r')\n",
    "    decode_file = open(os.path.join(config.PROCESSED_PATH, dec_filename), 'r')\n",
    "    encode, decode = encode_file.readline(), decode_file.readline()\n",
    "    data_buckets = [[] for _ in config.BUCKETS]\n",
    "    i = 0\n",
    "    while encode and decode:\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(\"Bucketing conversation number\", i+1)\n",
    "        encode_ids = [int(id_) for id_ in encode.split()]\n",
    "        decode_ids = [int(id_) for id_ in decode.split()]\n",
    "        for bucket_id, (encode_max_size, decode_max_size) in enumerate(config.BUCKETS):\n",
    "            if len(encode_ids) <= encode_max_size and len(decode_ids) <= decode_max_size:\n",
    "                data_buckets[bucket_id].append([encode_ids, decode_ids])\n",
    "                break\n",
    "        encode, decode = encode_file.readline(), decode_file.readline()\n",
    "        i += 1\n",
    "    return data_buckets\n",
    "#BIEN: Make input_ to have size=size by add some zero element to the end of input_\n",
    "def _pad_input(input_, size):\n",
    "    return input_ + [config.PAD_ID] * (size - len(input_))\n",
    "#BIEN: Following function will change the shape of inputs, example,\n",
    "#from 3x5 to 5x3, but not by an operation like matrix transpose \n",
    "# it takes all first element to make in an array, and second, and third...\n",
    "def _reshape_batch(inputs, size, batch_size):\n",
    "    \"\"\" Create batch-major inputs. Batch inputs are just re-indexed inputs\n",
    "    \"\"\"\n",
    "    batch_inputs = []\n",
    "    for length_id in range(size):\n",
    "        batch_inputs.append(np.array([inputs[batch_id][length_id]\n",
    "                                    for batch_id in range(batch_size)], dtype=np.int32))\n",
    "    return batch_inputs\n",
    "\n",
    "\n",
    "def get_batch(data_buckets, bucket_id, batch_size=1):\n",
    "    \"\"\" Return one batch to feed into the model \"\"\"\n",
    "    # only pad to the max length of the bucket\n",
    "    encoder_size, decoder_size = config.BUCKETS[bucket_id]\n",
    "    encoder_inputs, decoder_inputs = [], []\n",
    "\n",
    "    for _ in range(batch_size):\n",
    "        #BIEN: Choose randomly a couple of question-answer in the data_buckets\n",
    "        encoder_input, decoder_input = random.choice(data_buckets)\n",
    "        # pad both encoder and decoder, reverse the encoder\n",
    "        # So, encoder start by 0, 0, ...\n",
    "        encoder_inputs.append(list(reversed(_pad_input(encoder_input, encoder_size))))\n",
    "        decoder_inputs.append(_pad_input(decoder_input, decoder_size))\n",
    "\n",
    "    # now we create batch-major vectors from the data selected above.\n",
    "    batch_encoder_inputs = _reshape_batch(encoder_inputs, encoder_size, batch_size)\n",
    "    batch_decoder_inputs = _reshape_batch(decoder_inputs, decoder_size, batch_size)\n",
    "\n",
    "    # create decoder_masks to be 0 for decoders that are padding.\n",
    "    batch_masks = []\n",
    "    for length_id in range(decoder_size):\n",
    "        batch_mask = np.ones(batch_size, dtype=np.float32)\n",
    "        for batch_id in range(batch_size):\n",
    "            # we set mask to 0 if the corresponding target is a PAD symbol.\n",
    "            # the corresponding decoder is decoder_input shifted by 1 forward.\n",
    "            if length_id < decoder_size - 1:\n",
    "                target = decoder_inputs[batch_id][length_id + 1]\n",
    "            if length_id == decoder_size - 1 or target == config.PAD_ID:\n",
    "                batch_mask[batch_id] = 0.0\n",
    "        batch_masks.append(batch_mask)\n",
    "    return batch_encoder_inputs, batch_decoder_inputs, batch_masks\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    questions_train, answers_train, questions_test, answers_test = question_answers()\n",
    "    prepare_raw_data()\n",
    "    build_vocab()\n",
    "    process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= [1, 3, 5, 7]\n",
    "a[1:2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
