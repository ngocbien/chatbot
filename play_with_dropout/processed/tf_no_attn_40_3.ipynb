{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucketing conversation number 100\n",
      "Bucketing conversation number 200\n",
      "Bucketing conversation number 300\n",
      "Bucketing conversation number 400\n",
      "Bucketing conversation number 500\n",
      "Bucketing conversation number 100\n",
      "Initialize new model\n",
      "Create placeholders\n",
      "Creating loss...\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shapes (17, ?, 906) and (17, ?) are incompatible",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a6be2c37a168>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-a6be2c37a168>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(n_iters, lr)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChatBotModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebook/play_with_dropout/tf_model_no_attn.py\u001b[0m in \u001b[0;36mbuild_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_placeholders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m#self._inference()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_creat_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m        \u001b[0;31m# self._create_summary()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebook/play_with_dropout/tf_model_no_attn.py\u001b[0m in \u001b[0;36m_create_loss\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     64\u001b[0m                        \u001b[0moutput_projection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                        feed_previous= False)\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Time:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py\u001b[0m in \u001b[0;36msoftmax_cross_entropy\u001b[0;34m(onehot_labels, logits, weights, label_smoothing, scope, loss_collection, reduction)\u001b[0m\n\u001b[1;32m    730\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0monehot_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monehot_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m     \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monehot_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabel_smoothing\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \"\"\"\n\u001b[1;32m    763\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shapes %s and %s are incompatible\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shapes (17, ?, 906) and (17, ?) are incompatible"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_model_no_attn import ChatBotModel\n",
    "import config_tf as config\n",
    "import data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def run_step(sess, model, encoder_inputs, decoder_inputs,\n",
    "             decoder_masks,  forward_only):\n",
    "    encoder_size, decoder_size = config.BUCKETS\n",
    "    input_feed = {}\n",
    "    for step in range(encoder_size):\n",
    "        input_feed[model.encoder_inputs[step].name] = encoder_inputs[step]\n",
    "    for step in range(decoder_size):\n",
    "        input_feed[model.decoder_inputs[step].name] = decoder_inputs[step]\n",
    "        input_feed[model.decoder_masks[step].name] = decoder_masks[step]\n",
    "    \n",
    "    last_target = model.decoder_inputs[decoder_size].name\n",
    "    input_feed[last_target] = np.zeros([model.batch_size], dtype=np.int32)\n",
    "    if not forward_only:\n",
    "        output_feed = [model.train_ops,  # update op that does SGD.\n",
    "                       model.gradient_norms,  # gradient norm.\n",
    "                       model.losses]  # loss for this batch.\n",
    "    else:\n",
    "        output_feed = [model.losses]  # loss for this batch.\n",
    "        for step in range(decoder_size):  # output logits.\n",
    "            output_feed.append(model.outputs[step])\n",
    "    outputs = sess.run(output_feed, input_feed)\n",
    "    if not forward_only:\n",
    "        return outputs[1], outputs[2], None  # Gradient norm, loss, no outputs.\n",
    "    else:\n",
    "        return None, outputs[0], outputs[1:]  # No gradient norm, loss, outputs.\n",
    "\n",
    "def _get_data(train=True):\n",
    "    \n",
    "    #enc_vocab, dec_vocab, inv_enc_vocab, inv_dec_vocab = build_vocab()\n",
    "    if train:\n",
    "          DATA = data.load_data('question_train2id.txt', 'answer_train2id.txt')\n",
    "    else:\n",
    "          DATA = data.load_data('question_test2id.txt', 'answer_test2id.txt')                              \n",
    "    return  DATA\n",
    "\n",
    "def _get_skip_step(n_iters):\n",
    "    \"\"\" How many steps should the model train before it saves all the weights. \"\"\"\n",
    "\n",
    "    return int(n_iters/10)\n",
    "\n",
    "def _check_restore_parameters(sess, saver):\n",
    "    \"\"\" Restore the previously trained parameters if there are any. \"\"\"\n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.dirname(config.CPT_PATH_no_attn_40_3 + '/checkpoint'))\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        print(\"Loading parameters for the Chatbot\")\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        print(\"Initializing fresh parameters for the Chatbot\")\n",
    "\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "def train(n_iters, lr = config.LR):\n",
    "    \"\"\" Train the bot \"\"\"\n",
    "    data_buckets = training_data\n",
    "    tf.reset_default_graph() \n",
    "    model = ChatBotModel(False, config.BATCH_SIZE)\n",
    "    model.build_graph()\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        print('Running session')\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        _check_restore_parameters(sess, saver)\n",
    "\n",
    "        iteration = model.global_step.eval()\n",
    "        total_loss = 0\n",
    "        step =0\n",
    "        loss_print = []\n",
    "        bucket_id = 0\n",
    "        start = time.time()\n",
    "        while step < n_iters:\n",
    "            step+=1\n",
    "            skip_step = _get_skip_step(n_iters)\n",
    "            encoder_inputs, decoder_inputs, decoder_masks = data.get_batch(data_buckets,\n",
    "                                                                           batch_size=config.BATCH_SIZE)\n",
    "            \n",
    "            _, step_loss, _ = run_step(sess, model, encoder_inputs, \n",
    "                                       decoder_inputs, decoder_masks, False)\n",
    "            total_loss += step_loss\n",
    "            iteration += 1\n",
    "            if iteration % skip_step == 0:  \n",
    "                print_loss_avg = total_loss / skip_step\n",
    "                total_loss = 0\n",
    "                print('%s (%d %d%%) %.4f' % (timeSince(start, step / n_iters),\n",
    "                                         iteration, step / n_iters * 100, print_loss_avg))\n",
    "                total_loss = 0\n",
    "                loss_print.append(step_loss)\n",
    "        saver.save(sess, os.path.join(config.CPT_PATH_no_attn_40_3, 'chatbot'), global_step=model.global_step)\n",
    "        sys.stdout.flush()\n",
    "        plt.plot(loss_print)\n",
    "        plt.show()\n",
    "\n",
    "def _get_user_input():\n",
    "    t = input('Vous:  ')\n",
    "    return t\n",
    "\n",
    "def _construct_reponse(output_logits, dec_vocab):\n",
    "    outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
    "    if config.EOS_ID in outputs:\n",
    "        outputs = outputs[:outputs.index(config.EOS_ID)]\n",
    "\n",
    "    return \" \".join([tf.compat.as_str(dec_vocab[output]) for output in outputs])\n",
    "\n",
    "def chat():\n",
    "    _, enc_vocab = data.load_vocab(os.path.join(config.PROCESSED_PATH, 'encoder_vocab.txt'))\n",
    "    inv_dec_vocab, _= data.load_vocab(os.path.join(config.PROCESSED_PATH, 'decoder_vocab.txt'))\n",
    "    tf.reset_default_graph() \n",
    "    model = ChatBotModel(True, batch_size=1)\n",
    "    model.build_graph()\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        _check_restore_parameters(sess, saver)\n",
    "        output_file = open(os.path.join(config.PROCESSED_PATH, config.OUTPUT_FILE), 'a+')\n",
    "        max_length = config.BUCKETS[-1][0]\n",
    "        print('Bonjour, dites moi ce que vous voulez: ')\n",
    "        while True:\n",
    "            line = _get_user_input()\n",
    "            if len(line) > 0 and line[-1] == '\\n':\n",
    "                line = line[:-1]\n",
    "            if line == '':\n",
    "                break\n",
    "            output_file.write('VOUS ++++ ' + line + '\\n')\n",
    "            token_ids = data.sentence2id(enc_vocab, str(line))\n",
    "            if (len(token_ids) > max_length):\n",
    "                print('La longueur maximale est:', max_length)\n",
    "                continue\n",
    "            bucket_id = _find_right_bucket(len(token_ids))\n",
    "            encoder_inputs, decoder_inputs, decoder_masks = data.get_batch([(token_ids, [])], \n",
    "                                                                            bucket_id,\n",
    "                                                                            batch_size=1)\n",
    "            _, _, output_logits = run_step(sess, model, encoder_inputs, decoder_inputs,\n",
    "                                           decoder_masks, bucket_id, True)\n",
    "            response = _construct_reponse(output_logits, dec_vocab)\n",
    "            \n",
    "            print('Bot de AVICEN:  ', response)\n",
    "            output_file.write('BOT ++++ ' + response + '\\n')\n",
    "        output_file.write('=============================================\\n')\n",
    "        output_file.close()\n",
    "        \n",
    "def evaluate_randomly(n_iters=300, test=True):\n",
    "    if not test:\n",
    "        tf.reset_default_graph() \n",
    "        model = ChatBotModel(True, batch_size=1)\n",
    "        model.build_graph()\n",
    "        saver = tf.train.Saver()\n",
    "        list_print_random = random.sample(range(n_iters), 20)\n",
    "        total_loss = 0\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            _check_restore_parameters(sess, saver)\n",
    "            for i in range(n_iters):\n",
    "                random_index = random.choice(range(len(training_data)))\n",
    "                bucket_id = 0  \n",
    "                question = training_data[random_index][0]\n",
    "                answer = training_data[random_index][1]\n",
    "                encoder_inputs, decoder_inputs, decoder_masks = \\\n",
    "                    data.get_batch([(question, [])], \\\n",
    "                    bucket_id, batch_size=1)\n",
    "                _, _, output_logits = run_step(sess, model, encoder_inputs, decoder_inputs,\n",
    "                                           decoder_masks, bucket_id, True)\n",
    "                reponse = _construct_reponse(output_logits, dec_vocab)\n",
    "                bonne_reponse = \" \".join([str(dec_vocab[id]) for id in answer])\n",
    "                question = \" \".join([str(enc_vocab[id]) for id in question])\n",
    "                loss = _evaluate_by_right_word(reponse, bonne_reponse)\n",
    "                total_loss +=loss\n",
    "                if i in list_print_random:\n",
    "                    print('-'*50)\n",
    "                    print('Question:  ',  question)\n",
    "                    print('Bot     :  ', reponse)\n",
    "                    print('RÃ©ponse: {}  ACCURACY {}'.format(bonne_reponse, 1-loss))\n",
    "        print('Test on {} sentences in train set with accuracy {}'.format(n_iters,1- total_loss/n_iters))\n",
    "    else:\n",
    "        tf.reset_default_graph() \n",
    "        model = ChatBotModel(True, batch_size=1)\n",
    "        model.build_graph()\n",
    "        saver = tf.train.Saver()\n",
    "        total_loss = 0\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            _check_restore_parameters(sess, saver)\n",
    "            n_iters = len(test_data)\n",
    "            print_random_index = random.sample(range(n_iters), 20)\n",
    "            bucket_id = 0\n",
    "            for i in range(n_iters):\n",
    "                question = test_data[i][0]\n",
    "                answer = test_data[i][1]\n",
    "                encoder_inputs, decoder_inputs, decoder_masks = data.get_batch([(question, [])], \n",
    "                                                                            bucket_id,\n",
    "                                                                            batch_size=1)\n",
    "                _, _, output_logits = run_step(sess, model, encoder_inputs, decoder_inputs,\n",
    "                                           decoder_masks, bucket_id, True)\n",
    "                reponse = _construct_reponse(output_logits, dec_vocab)\n",
    "                bonne_reponse = \" \".join([str(dec_vocab[id]) for id in answer[1:-1]])\n",
    "                loss = _evaluate_by_right_word(reponse, bonne_reponse)\n",
    "                total_loss +=loss\n",
    "                if i in print_random_index:\n",
    "                    question = \" \".join([str(enc_vocab[id]) for id in question])\n",
    "                    print('Question: ', question)\n",
    "                    print('Reponse: ', reponse)\n",
    "                    print('Bonne Reponse: {}. ACCURACY {:.1f} '.format(bonne_reponse,1-loss))\n",
    "                    print('-'*50)\n",
    "        print('Test on {} sentences'.format(n_iters))\n",
    "        print('Accuracy by percent of true words {}'.format(1-total_loss/n_iters))\n",
    "        #return loss/n_iters\n",
    "\n",
    "def _evaluate_by_right_word(reponse, bonne_reponse):\n",
    "    reponse = reponse.split()\n",
    "    bonne_reponse = bonne_reponse.split()\n",
    "    min_length = min(len(reponse), len(bonne_reponse))\n",
    "    max_length = max(len(reponse), len(bonne_reponse))\n",
    "    error = max_length-min_length\n",
    "    for i in range(min_length):\n",
    "        if reponse[i] != bonne_reponse[i]:\n",
    "            error +=1\n",
    "    return error/max_length\n",
    "\n",
    "def _test_data(n =20):\n",
    "    list_random = random.sample(range(len(DATA)), 10)\n",
    "    for i in list_random:\n",
    "        pair = DATA[i]\n",
    "        question = ' '.join([enc_vocab[ix] for ix in pair[0]])\n",
    "        answer = ' '.join([dec_vocab[ix] for ix in pair[1]])\n",
    "        print(i, ': ', question)\n",
    "        print(i, ': ',answer)\n",
    "        #print('-'*30)\n",
    "\n",
    "if __name__ =='__main__':\n",
    "    try:\n",
    "        t_training_data\n",
    "    except NameError:\n",
    "        enc_vocab,  inv_enc_vocab = data.load_vocab('processed/encoder_vocab.txt')\n",
    "        dec_vocab,  inv_dec_vocab = data.load_vocab('processed/decoder_vocab.txt')\n",
    "        training_data = _get_data(True)\n",
    "        test_data = _get_data(False)\n",
    "    \n",
    "train(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(2, 3)\n"
     ]
    }
   ],
   "source": [
    "a, b = [1, 2], [2, 3, 5]\n",
    "c = zip(a,b)\n",
    "for i in c:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
