{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/gensim/models/ldamodel.py:775: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1303: RuntimeWarning: invalid value encountered in sqrt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 2096 couples and test on 541 couples\n",
      "on test, nous avons les scores sont pre 0.50906 recall 0.51397 et F 0.51150\n",
      "on train, nous avons, precision 0.64109 recall 0.64615 et F 0.64361\n",
      "all time is  66  seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "les symboles particulières comme ? ! et . sont supprimé dans la question.\n",
    "Il faut peut être returner une réponse spécifique si il rencontre plus 1 mot qu'il existe pas dans le corpus\n",
    "Nous suprimmons également:\n",
    "  1. Mot qui contient une seule lettre(qui sont suivante des erreurs)\n",
    "  2. Une fonction unique pour nettoyage tous les données dans train-test et chat\n",
    "Nous allons traiter tous les mots clés suivant :\n",
    "km, vitesse, carburant, huile, position, batterie, pression_pneu. Il reste trajet et vin\n",
    "\n",
    "Méthode TF_IDF donne un résultat 72.8% on train et 53.7% on test, qui est un résultat faible. Nous \n",
    "cherchons une autre méthode pour espérer un meilleur résultat...\n",
    "  \n",
    "\"\"\"\n",
    "#import sys\n",
    "#!{sys.executable} -m pip uninstall gtts\n",
    "\n",
    "#from gtts import gTTS\n",
    "#from playsound import playsound\n",
    "#import speech_recognition\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = stopwords.words('french')\n",
    "from nltk.stem import *\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"french\", ignore_stopwords = False)\n",
    "import csv\n",
    "import re\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "from dateparser.search import search_dates\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import LdaModel\n",
    "from gensim import models, corpora, similarities\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def date_time_():\n",
    "    \n",
    "    DICT_NUMERO = {'ce':1, 'cette':1, 'un':1, 'une':1, 'deux':2, 'trois':3,\\\n",
    "                   'quatre':4, 'cinq':5, 'six':6, 'sept':7,\\\n",
    "              'huit':8, 'neuf':9, 'dix':10, 'onze':11, 'douze':12, 'treize':13,'quartoze':14,\\\n",
    "               'quinze':15, 'seize':16, 'dix-sept':17,'dix sept':17, 'dix huit':18, \\\n",
    "               'dix-huit':18, 'dix-neuf':19, 'vingt':20, 'ving et un':21, 'trente':30, 'quarante':40,\\\n",
    "                  'quarante cinq':45, 'soixante':60}\n",
    "    DICT_TEMPS = {'heure':1, 'matin':4, 'après-midi':2,  'journée':24,  'jour':24, \\\n",
    "              'mois':30*24, 'semaine':7*24, \\\n",
    "              'an':24*365, 'année':24*365, 'minute': 1/60}\n",
    "    DICT_COMPLET_TEMPS = {}\n",
    "    for temps in DICT_TEMPS:\n",
    "        for numéro in DICT_NUMERO:\n",
    "            key1 = numéro + ' '+temps\n",
    "            key2 = str(DICT_NUMERO[numéro]) + ' ' +temps\n",
    "            value = DICT_NUMERO[numéro]*DICT_TEMPS[temps]\n",
    "            DICT_COMPLET_TEMPS[key1] = value\n",
    "            DICT_COMPLET_TEMPS[key2] = value\n",
    "    DICT_FOR_SPECIAL_CASE = {'aujourd\\'hui':8, 'hébdomadaire':7*24, 'annuel': 365*24, \\\n",
    "                           'hier': 24, 'avant-hier': 48, 'annuelle': 365*24 , 'actuellement':1/2,\\\n",
    "                            'maintenant':1/2}\n",
    "    DICT_COMPLET_TEMPS['#cas_particulier#'] = DICT_FOR_SPECIAL_CASE\n",
    "    DICT_COMPLET_TEMPS['avant hier'] = 72\n",
    "    DICT_COMPLET_TEMPS['semaine dernière'] = 24*7\n",
    "    return DICT_COMPLET_TEMPS\n",
    "\n",
    "def construct_list_city():\n",
    "    #import csv\n",
    "    liste_ville =[]\n",
    "    with open('data/villes_france.csv') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            liste_ville.append(row[2])\n",
    "        for ville in liste_ville:\n",
    "            if len(ville) <4:\n",
    "                liste_ville.remove(ville)\n",
    "    return liste_ville\n",
    "\n",
    "def _find_right_time(question, Dict):\n",
    "    \n",
    "    list_ = ['?', '.', ' ', '\\n', '!']\n",
    "    for symbol in list_:\n",
    "        question = question.lower().strip(symbol)\n",
    "    list_word = question.split()\n",
    "    word_ = None\n",
    "    list_ = []\n",
    "    if len(list_word) < 2:\n",
    "        return None, None \n",
    "    for word in list_word:\n",
    "        if word_ is not None:\n",
    "            list_.append(str(word_)+' '+str(word))  \n",
    "        word_ = word\n",
    "    for key in Dict:\n",
    "        for word in  list_:\n",
    "            if key == word:\n",
    "                return Dict[key], key\n",
    "    for word in Dict['#cas_particulier#']:\n",
    "        for word2 in question.split():\n",
    "            if word2 == word:\n",
    "                return Dict['#cas_particulier#'][word], word       \n",
    "    return None, None\n",
    "\n",
    "\n",
    "def _find_right_geography(question, Dict):\n",
    "    \"\"\"\n",
    "    Nous cherchons une ville, un département dans le dictionnaire pour retourner soit\n",
    "    ce nom de ce lieu, soit ses coordonnées correspondance. Le retour de ce donnée dépend \n",
    "    de quel façon nous allons traiter avec cette information.\n",
    "    \n",
    "    \"\"\"\n",
    "    list_ = ['?', '.', '\\n', '!', ',']\n",
    "    for symbol in list_:\n",
    "        question = question.replace(symbol, '') \n",
    "    list_geography = []\n",
    "    for word in question.split():\n",
    "        if word.lower() !=word:\n",
    "            word1 = word.lower()\n",
    "            if word1 in Dict:\n",
    "                list_geography.append(word)\n",
    "    if len(list_geography) >0:\n",
    "        return list_geography\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def list_variable():\n",
    "    \"\"\"\n",
    "    list_var is to manually complete \"\"\"\n",
    "    list_var = []\n",
    "    if len(list_var) >0:\n",
    "        return list_var\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def _normalized_table():\n",
    "    \"\"\"\n",
    "    return a data frame with new name\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv('fleet_donnees.csv', sep = ';')\n",
    "    variable_names = list_variable()\n",
    "    if variable_names ==None:\n",
    "        return df\n",
    "    else:\n",
    "        name = list(df)\n",
    "        dict_name = {name[i]: variable_names[i] for i in range(len(name))}\n",
    "        df = df.rename(index= str, columns = dict_name)\n",
    "        return df\n",
    "\n",
    "def find_right_time_from_table(time, df):\n",
    "    \"\"\"\n",
    "    given time found from a question, match it with the time given in table\n",
    "    This depend on format of time in table.\n",
    "    \n",
    "    \"\"\"\n",
    "    return None\n",
    "    \n",
    "    \n",
    "def clearing_word(word):\n",
    "    word = re.sub('\\x8e', 'é', word)\n",
    "    word = re.sub('\\x88', 'à', word)\n",
    "    word = re.sub('\\x9d', 'ù', word)\n",
    "    word = re.sub('\\x8f', 'è', word)\n",
    "    word = re.sub('\\x9e', 'û', word)\n",
    "    word = re.sub('\\x90', 'ê', word)\n",
    "    word = re.sub('\\x99', 'ô', word)\n",
    "    word = re.sub('\\x94', 'î', word)\n",
    "   # word = re.sub('\\x8f', 'è', word)\n",
    "    word = re.sub('\\x8d', 'ç', word)\n",
    "    word = re.sub('õ', '', word)\n",
    "    word = re.sub('Ê', '', word)\n",
    "    word = re.sub('[?,.,!, \\,,  %]', '', word)\n",
    "    if word == 'û' or word == 'v' or word == 'é':\n",
    "        word = ''\n",
    "    if word ==\"2017êles\":\n",
    "        word = \"2017\"\n",
    "    if \"ênox\" in word:\n",
    "        word =\"nox\"\n",
    "    return word\n",
    "\n",
    "\n",
    "def clear_line(pharse, stop_word):\n",
    "    \"\"\"\n",
    "    Arg: string\n",
    "    Return: string\n",
    "    \"\"\"\n",
    "    dict_manuel = dict_manuelle()\n",
    "    pharse_ =[]\n",
    "    for words in pharse.strip().strip('?').strip('.').strip('!').lower().split(' '):\n",
    "        for word in words.split('-'):\n",
    "            for word_ in word.split('\\''):\n",
    "                word_ = clearing_word(word_)\n",
    "                word_ = stemmer.stem(word_)\n",
    "        if word_ not in stop_word and len(word_) >1:       \n",
    "            pharse_.append(word_)  \n",
    "        for i in range(len(pharse_)):\n",
    "            for line in dict_manuel:\n",
    "                if pharse_[i] in line:\n",
    "                    pharse_[i] = line[0]\n",
    "    return ' '.join(pharse_)\n",
    "\n",
    "def stemmer_line(line):\n",
    "    return ' '.join([stemmer.stem(word) for word in line.split()])\n",
    "\n",
    "def prepareData(PAIRS):\n",
    "    pairs_trains, pairs_tests = [], []\n",
    "   # PAIRS.extend(pairs1)\n",
    "    index_train = random.sample(range(len(PAIRS)), int(len(PAIRS)*0.80))\n",
    "    for i in range(len(PAIRS)):\n",
    "        if i in index_train:\n",
    "            pairs_trains.append(PAIRS[i])\n",
    "        else:\n",
    "            pairs_tests.append(PAIRS[i])\n",
    "    print(\"Read %s sentence pairs of training set\" % len(pairs_trains))\n",
    "    print(\"Read %s sentence pairs of test set\" % len(pairs_tests))\n",
    "    return pairs_trains, pairs_tests\n",
    "\n",
    "def get_all_convos(stopwords, file):\n",
    "    \n",
    "    convos = []\n",
    "    questions_to_print = []\n",
    "    liste_file = [file]\n",
    "    list_of_word = []\n",
    "    for file in liste_file:\n",
    "        with open(file) as f:\n",
    "            i=0\n",
    "            for line in f:\n",
    "                if i%2==0:\n",
    "                    question = str(line)\n",
    "                    if '++++' in question:\n",
    "                         question = question[8:]\n",
    "                    question_ = question.strip().strip('!').strip('?').strip('.')\n",
    "                    questions_to_print.append(question_)\n",
    "                    question = clear_line(question, stopwords)\n",
    "                else:\n",
    "                    answer = str(line)\n",
    "                    if '++++' in answer:\n",
    "                        answer = answer[8:].strip()\n",
    "                    convos.append([question, answer])\n",
    "                i+=1\n",
    "        f.close()\n",
    "        good_index = []\n",
    "        convos_ = []\n",
    "        for i in range(len(convos)):\n",
    "            if convos[i][0] not in convos_:\n",
    "                convos_.append(convos[i][0])\n",
    "                good_index.append(i)\n",
    "            \n",
    "    \n",
    "    return [convos[i] for i in good_index], [questions_to_print[i] for i in good_index]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_all_questions_to_print(file):\n",
    "    \n",
    "    convos = []\n",
    "    liste_file = [file]\n",
    "    list_of_word = []\n",
    "    for file in liste_file:\n",
    "        with open(file) as f:\n",
    "            i=0\n",
    "            for line in f:\n",
    "                if i%2==0:\n",
    "                    question = str(line)\n",
    "                    if '++++' in question:\n",
    "                         question = question[8:]\n",
    "                    question = question.strip()\n",
    "                    convos.append(question)\n",
    "                i+=1\n",
    "        f.close()\n",
    "        indexes = []\n",
    "        index = 0\n",
    "    for question in convos:\n",
    "        question = clear_line(question, STOPWORDS)\n",
    "        if len(question) !=0:\n",
    "                indexes.append(index)\n",
    "        index +=1\n",
    "    return [convos[i] for i in indexes]\n",
    "\n",
    "def get_list_words(convos):\n",
    "    list_words =[]\n",
    "    for pair in convos:\n",
    "        for word in pair[0].split():\n",
    "            if word not in list_words:\n",
    "                list_words.append(word)\n",
    "    print('There are {} words in corpus:'.format(len(list_words)))\n",
    "    return list_words\n",
    "                \n",
    "\n",
    "def simulation_pairs():\n",
    "    \n",
    "    DIRECT_VARIABLE = ['vitesse', 'batterie', 'position', 'km']\n",
    "    BORDE = {'MAX': ['plus grand', 'plus grande', 'plus grands', \n",
    "         'plus grandes',\n",
    "         'plus vite', 'max', 'maximum', 'maximal', 'maximaux', 'maximale',\n",
    "        'plus haut', 'plus haute', 'plus hautes', 'plus hauts',  \n",
    "        'plus élevé', 'plus élevée', 'plus élevés', 'plus élevées'], \n",
    "         'MIN':['moins grand', 'moins grande', 'moins grands','moins grandes',\n",
    "        'plus petit', 'plus petite', 'plus petits', 'plus petites','plus faible', 'plus faibles', \n",
    "         'min', 'minimal', 'minimale', 'minimales', 'minimaux','moins vite', \n",
    "         'moins élevé', 'moins élevée', 'moins élevés', 'moins élevées'],\n",
    "        'MOYENNE': ['moyenne', 'moyen', 'moyennement']\n",
    "        }\n",
    "    COMPLEX_ANALYSIS = ['problème', 'problèmes', 'erreurs', 'erreur', 'danger']\n",
    "    PAIRS = []\n",
    "    question_for_simulation = ['quels véhicules ont', 'quel véhicule', 'quelle voiture', 'quelles voitures']\n",
    "    reponse_for_simulation = 'véhicule'\n",
    "    for borde in BORDE:\n",
    "        for word in BORDE[borde]:\n",
    "            for direct_variable in DIRECT_VARIABLE:\n",
    "                for question_f_s in question_for_simulation:\n",
    "                    if direct_variable !='position':\n",
    "                        question= question_f_s+' '+direct_variable + ' '+ word\n",
    "                        key0 = '#id#'\n",
    "                        key1 = '#'+direct_variable+'#'\n",
    "                        key2 = '#'+borde +'#'\n",
    "                        reponse = reponse_for_simulation +' '+key0+ ' '+ key1+ ' '+ key2\n",
    "                        PAIRS.append([question,reponse])\n",
    "\n",
    "    question_for_simulation = ['', 'quel véhicule a', 'quelle voiture a', 'quelles voitures ont', 'il y a']\n",
    "    reponse_for_simulation = 'véhicule' \n",
    "    for direct_variable in DIRECT_VARIABLE:\n",
    "        for complex_analysis in COMPLEX_ANALYSIS:\n",
    "            for question_f_s in question_for_simulation:\n",
    "                #if direct_variable !='position':\n",
    "                      question= question_f_s+' '+complex_analysis + ' '+ direct_variable\n",
    "                      key0 = '#id#'\n",
    "                      key1 = '#'+direct_variable+'#'\n",
    "                      key2 = '#'+ complex_analysis +'#'\n",
    "                      reponse = reponse_for_simulation +' '+key0+ ' '+ key2 +' '+ key1\n",
    "                      PAIRS.append([question,reponse])\n",
    "    return PAIRS\n",
    "\n",
    "def clear_pairs_trains(pairs_trains):\n",
    "    questions = []\n",
    "    index = []\n",
    "    for i in range(len(pairs_trains)):\n",
    "        if  pairs_trains[i][0] not in questions:\n",
    "            index.append(i)\n",
    "            questions.append(pairs_trains[i][0])\n",
    "    return [pairs_trains[i] for i in index]\n",
    "\n",
    "def tf_idf(word, doc, méthode):\n",
    "    tf, df = 0, 0\n",
    "    len_doc = len(doc.split())\n",
    "    value_doc = 1/math.sqrt(len_doc)\n",
    "    value_doc = math.sqrt(value_doc)\n",
    "    if méthode !='Lucene':\n",
    "        value_doc = 1\n",
    "    if word in KEY_LIST:\n",
    "        value_word = 10*value_doc\n",
    "    else:\n",
    "        value_word = 1*value_doc\n",
    "    if word in str(doc):\n",
    "        tf = value_word\n",
    "    for doc_ex in QUESTION_TO_TRAINS:\n",
    "        if word in str(doc_ex):\n",
    "            df+=1\n",
    "    if méthode  == 'normal':\n",
    "        if df>0:\n",
    "            return tf*math.log(len(QUESTION_TO_TRAINS)/(df), 10)\n",
    "        else:\n",
    "            return 0\n",
    "    if méthode == 'probabiliste':\n",
    "        if df>0:\n",
    "            return tf*math.log((len(QUESTION_TO_TRAINS)-df)/(df), 10)\n",
    "        else:\n",
    "            return 0\n",
    "    if méthode == 'lissé':\n",
    "        if df>0:\n",
    "            return tf*(1+math.log(len(QUESTION_TO_TRAINS)/df, 10))\n",
    "        else:\n",
    "            return tf\n",
    "    if méthode == 'probabiliste_lissé':\n",
    "        if df>0:\n",
    "            return tf*(1+math.log((len(QUESTION_TO_TRAINS)-df)/(df), 10))\n",
    "        else:\n",
    "            return tf\n",
    "\n",
    "def dict_doc_score(doc, méthode = 'normal'): \n",
    "    \"\"\"\n",
    "    We return score of all one words and all two consequent words\"\"\"\n",
    "\n",
    "    list_word_ = split_words(doc)    \n",
    "    return {word: tf_idf(word, doc, méthode) for word in list_word_}\n",
    "    \n",
    "def dict_doc_score_for_classif(doc, sujet): \n",
    "    \"\"\"\n",
    "    We return score of all one words and all two consequent words\"\"\"\n",
    "\n",
    "    list_word_ = split_words(doc)    \n",
    "    return {word: tf_idf_for_classif(word, doc, sujet) for word in list_word_}\n",
    "\n",
    "   \n",
    "def split_words(doc):\n",
    "    \n",
    "    list1 = doc.split()\n",
    "    list2 = []\n",
    "    for word in list1:\n",
    "        list2.append(word)\n",
    "    for i in range(len(list1)-1):\n",
    "        word = list1[i]+ ' '+list1[i+1]\n",
    "        list2.append(word)\n",
    "    if len(list1)>2:\n",
    "        for i in range(len(list1)-2):\n",
    "            word = list1[i]+' '+list1[i+1]+' '+list1[i+2]\n",
    "            list2.append(word)\n",
    "    return list2\n",
    "        \n",
    "        \n",
    "def get_key_list(pairs_trains):\n",
    "    \n",
    "    dict_ = dict_manuelle()\n",
    "    KEY_LIST = []\n",
    "    KEY_LIST_ = []\n",
    "    for pair in pairs_trains:\n",
    "        for word in pair[1].split():\n",
    "            if '#' in word:\n",
    "                word = re.sub('#', '', word)\n",
    "                if word not in KEY_LIST:\n",
    "                    KEY_LIST.append(word)\n",
    "    for key in KEY_LIST:\n",
    "        key = stemmer.stem(key)\n",
    "        if key not in KEY_LIST_ and '_' not in key:\n",
    "            KEY_LIST_.append(key)\n",
    "    KEY_RETURN = []\n",
    "    for key in KEY_LIST_:\n",
    "        boolean = False\n",
    "        for list_ in dict_:\n",
    "            if key in list_:\n",
    "                KEY_RETURN.append(list_[0])\n",
    "                boolean = True\n",
    "                break\n",
    "        if not boolean:\n",
    "            KEY_RETURN.append(key)\n",
    "    #for list_ in dict_:\n",
    "       # if list_[0] not in KEY_RETURN:\n",
    "        #    KEY_RETURN.append(list_[0])       \n",
    "    return KEY_RETURN\n",
    "        \n",
    "\n",
    "def find_right_index(new_doc,DICT, méthode = 'normal'):\n",
    "    \n",
    "    index=0\n",
    "    biggest_score = score_by_new_doc(new_doc, QUESTION_TO_TRAINS[0],DICT, méthode)\n",
    "    for i in range(len(QUESTION_TO_TRAINS)):\n",
    "        boolean1 = score_by_new_doc(new_doc, QUESTION_TO_TRAINS[i], DICT, méthode) >biggest_score\n",
    "        boolean2 = score_by_new_doc(new_doc, QUESTION_TO_TRAINS[i], DICT, méthode) == biggest_score\n",
    "        boolean3 = len(QUESTION_TO_TRAINS[index]) >len(QUESTION_TO_TRAINS[i])\n",
    "        if boolean1 or (boolean2 and boolean3):\n",
    "            biggest_score = score_by_new_doc(new_doc, QUESTION_TO_TRAINS[i],DICT, méthode)\n",
    "            index = i\n",
    "    précision, rappel = precision_recall(new_doc, QUESTION_TO_TRAINS[index])\n",
    "    #if biggest_score ==0 or précision <0.2 or rappel<0.2:\n",
    "     #   for question in QUESTION_TO_TRAINS:\n",
    "      #      if '_return_' in question:\n",
    "       #         try:\n",
    "        #            index = QUESTION_TO_TRAINS.index(question)\n",
    "         #       except ValueError:\n",
    "          #          index = -1\n",
    "    return index\n",
    "\n",
    "def reformule_question_if_need(new_question, good_question):\n",
    "    \n",
    "    if 'to_return' in good_question:\n",
    "        return True\n",
    "    score1 = score_by_new_doc(new_question, good_question, DICT)\n",
    "    score2 = 0\n",
    "    for word in Dict_of_methode[good_question]:\n",
    "        score2 += Dict_of_methode[good_question][word]\n",
    "    if score1/score2 > 0.6:\n",
    "        return  True\n",
    "    else:\n",
    "        return  False\n",
    "\n",
    "\n",
    "def test(DICT, méthode, train =False):\n",
    "    \n",
    "    if not train:\n",
    "        index_to_print = random.sample(range(len(pairs_tests)), 20)\n",
    "        #loss_total = 0\n",
    "        for i in range(len(pairs_tests)):\n",
    "            index = find_right_index(pairs_tests[i][0], DICT, méthode)\n",
    "            #loss = _evaluate_by_right_word(pairs_trains[index][1], pairs_tests[i][1])\n",
    "            #loss_total +=loss\n",
    "            if i in index_to_print:\n",
    "                print(pairs_tests[i])\n",
    "                print(pairs_trains[index][1])\n",
    "        print('-'*80)\n",
    "       # print('ACCURACY=', 1-loss_total/len(pairs_tests))\n",
    "    if  train:\n",
    "        #loss_total = 0\n",
    "        #index_to_print = random.sample(range(len(pairs_trains)), 20)  \n",
    "        list_of_bad_prediction = []\n",
    "        i0 = 0\n",
    "        for i in range(len(pairs_trains)):\n",
    "            index = find_right_index(pairs_trains[i][0], DICT, méthode)\n",
    "            #loss = _evaluate_by_right_word(pairs_trains[index][1], pairs_trains[i][1])\n",
    "            #loss_total +=loss\n",
    "            if index != i :\n",
    "                i0 +=1\n",
    "                print(pairs_trains[i])\n",
    "                print(pairs_trains[index][1])\n",
    "        print('-'*80)\n",
    "        #print('ACCURACY=', 1-loss_total/len(pairs_trains))\n",
    "        print('Il y a {} erreurs d\\'indexes parmis {} prédictions'.format(i0, len(pairs_trains)))\n",
    "\n",
    "        \n",
    "def test_without_print(DICT, méthode, train =False):\n",
    "    \n",
    "    if not train:\n",
    "        loss_total = 0\n",
    "        for i in range(len(pairs_tests)):\n",
    "            index = find_right_index(pairs_tests[i][0], DICT, méthode)\n",
    "            loss = _evaluate_by_right_word(pairs_trains[index][1], pairs_tests[i][1])\n",
    "            loss_total +=loss\n",
    "        print('-'*80)\n",
    "        print('ACCURACY=', 1-loss_total/len(pairs_tests))\n",
    "    if  train:\n",
    "        loss_total = 0\n",
    "        index_to_print = random.sample(range(len(pairs_trains)), 20)   \n",
    "        for i in range(len(pairs_trains)):\n",
    "            index = find_right_index(pairs_trains[i][0], DICT, méthode)\n",
    "            loss = _evaluate_by_right_word(pairs_trains[index][1], pairs_trains[i][1])\n",
    "            loss_total +=loss\n",
    "        print('-'*80)\n",
    "        print('ACCURACY=', 1-loss_total/len(pairs_trains))\n",
    "\n",
    "def change_subjet_of_question(question):\n",
    "    dict_ = {'ton':'mon', 'ta':'ma', 'votre':'mon', 'tes':'mes', 'mon':'ton', \\\n",
    "            'ma':'ta', 'mes':'tes', 'tu': 'je', 'moi':'toi', 'je':'tu', 'toi':'moi',\\\n",
    "            'vous':'je'}\n",
    "    question_ = []\n",
    "    for word in question.lower().split():\n",
    "        if word in dict_:\n",
    "            word = dict_[word]\n",
    "        question_.append(word)\n",
    "    return ' '.join(question_)\n",
    "        \n",
    "def chat(méthode='normal'):\n",
    "    \n",
    "    print('Bonjour, C\\'est le bot d\\'Avicen, pose tes questions, s\\'il te plaît!')\n",
    "    path = 'processed/chat_tfidf.txt'\n",
    "    f =  open(path, 'a+') \n",
    "    while True:\n",
    "            line = str(input('Vous: '))\n",
    "            list_city = _find_right_geography(line, CITY_LIST)\n",
    "            line = line.lower().strip().strip('?').strip('.')\n",
    "            time, time_word = _find_right_time(line, COMPLET_DICT_TIME)\n",
    "            if time_word is not None:\n",
    "                line = line.replace(time_word, '')\n",
    "            if list_city is not None:\n",
    "                for city in list_city:\n",
    "                    line = line.replace(city, '')\n",
    "            LINE = clear_line(line, STOPWORDS)\n",
    "            unknown_word =0\n",
    "            for word in LINE.split():\n",
    "                if word not in LIST_OF_WORDS:\n",
    "                    unknown_word += 1\n",
    "            #if unknown_word > 1:\n",
    "             #   print('Désolé, je ne comprends pas ta question, peux tu la reformuler s\\'il te plaît?')\n",
    "              #  continue\n",
    "            if len(line) !=0:\n",
    "                index= find_right_index(LINE, DICT, méthode)\n",
    "                good_question = QUESTION_TO_TRAINS[index]\n",
    "                if not reformule_question_if_need(LINE, good_question):\n",
    "                    get_answer = str(input('Tu veux demander: {}?  |'.format(questions_sujet[index])))\n",
    "                    if 'non' in get_answer.lower():\n",
    "                        print('Peux tu reformuler ta question s\\'il te plaît?')\n",
    "                        continue\n",
    "                if time == None and list_city == None:\n",
    "                     print('bot: ', pairs_trains[index][1])\n",
    "                elif list_city ==None:\n",
    "                    print('Bot: {}, {}'.format(time_word, pairs_trains[index][1]))\n",
    "                elif time == None:\n",
    "                    cities = ' '.join(list_city)\n",
    "                    print('Bot: À {}, {}'.format(cities, pairs_trains[index][1]))\n",
    "                else:\n",
    "                    cities = ' '.join(list_city)\n",
    "                    print('Bot: À {}, {}, {}'.format(cities, time_word, pairs_trains[index][1]))\n",
    "                f.write('VOUS ++++ '+line+'\\n')\n",
    "                f.write('BOT ++++ '+pairs_trains[index][1]+'\\n')\n",
    "            \n",
    "            else:\n",
    "                f.close()\n",
    "                break \n",
    "\n",
    "def precision_recall(lstcomp, lstref):\n",
    "    \n",
    "    card_intersec = 0.0 # force à utiliser la division non entière\n",
    "    for t in set(lstcomp) :\n",
    "        card_intersec += min(lstref.count(t), lstcomp.count(t))\n",
    "    if len(lstcomp)==0:\n",
    "        precision =1\n",
    "    else:\n",
    "        precision = card_intersec/len(lstcomp)\n",
    "    if len(lstref)==0:\n",
    "        rappel = 1\n",
    "    else:\n",
    "        rappel = card_intersec/len(lstref)\n",
    "    return (precision, rappel)\n",
    "\n",
    "def answer_from_index(index, sujet):\n",
    "    if sujet:\n",
    "        convos = convos_sujet\n",
    "    else:\n",
    "        convos = convos_hors\n",
    "    return convos[index][1]\n",
    "\n",
    "def question_from_index(index, sujet):\n",
    "    if sujet:\n",
    "        convos = convos_sujet\n",
    "    else:\n",
    "        convos = convos_hors\n",
    "    return convos[index][0]\n",
    "\n",
    "\n",
    "               \n",
    "def dict_of_all_score(METHODE):\n",
    "    \"\"\"\n",
    "    return score of all word in all doc\"\"\"\n",
    "    DICT = {}\n",
    "    for méthode in METHODE:\n",
    "        dict_for_this_method ={}\n",
    "        for doc in QUESTION_TO_TRAINS:\n",
    "            dict_for_this_method[doc]=dict_doc_score(doc, méthode)\n",
    "        DICT[méthode] = dict_for_this_method\n",
    "    return DICT\n",
    " \n",
    "    \n",
    "def chatting_with_corpus():\n",
    "\n",
    "    path = 'processed/chat_tfidf.txt'\n",
    "    i = 0\n",
    "    questions = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            if i%2 ==0:\n",
    "                line = line[9:]\n",
    "                line = line.strip()\n",
    "                questions.append(line)\n",
    "            i +=1\n",
    "    f.close()\n",
    "    good_index = []\n",
    "    questions_ = []\n",
    "    index = 0\n",
    "    for question in questions:\n",
    "        question = clear_line(question, STOPWORDS)\n",
    "        if question not in questions_:\n",
    "            questions_.append(question)\n",
    "            good_index.append(index)\n",
    "        index += 1\n",
    "    questions_clean = [questions[i] for i in good_index]\n",
    "    for line in questions_clean:\n",
    "            print('Humaine: ', line)\n",
    "            time, time_word = _find_right_time(line, COMPLET_DICT_TIME)\n",
    "            list_city = _find_right_geography(line, CITY_LIST)\n",
    "            if time_word is not None:\n",
    "                line = line.replace(time_word, '')\n",
    "            if list_city is not None:\n",
    "                for city in list_city:\n",
    "                    line = line.replace(city, '')\n",
    "            LINE = clear_line(line, STOPWORDS)\n",
    "            unknown_word =0\n",
    "            for word in LINE.split():\n",
    "                if word not in LIST_OF_WORDS:\n",
    "                    unknown_word += 1\n",
    "            if unknown_word > 1:\n",
    "                print('Désolé, je ne comprends pas ta question, peux tu la reformuler s\\'il te plaît?')\n",
    "                print()\n",
    "                continue\n",
    "            if len(line) !=0:\n",
    "                index= find_right_index(LINE, DICT, méthode)\n",
    "                if time == None and list_city == None:\n",
    "                     print('Bot: ', pairs_trains[index][1])\n",
    "                elif list_city ==None:\n",
    "                    print('{}, {}'.format(time_word, pairs_trains[index][1]))\n",
    "                elif time == None:\n",
    "                    cities = ' '.join(list_city)\n",
    "                    print('À {}, {}'.format(cities, pairs_trains[index][1]))\n",
    "                else:\n",
    "                    cities = ' '.join(list_city)\n",
    "                    print('À {}, {}, {}'.format(cities, time_word, pairs_trains[index][1]))\n",
    "            print()\n",
    "               \n",
    "def dict_manuelle():\n",
    "    \"\"\"\n",
    "    return la liste des listes des synonymes\"\"\"\n",
    "    list_ = []\n",
    "    dict_ = {}\n",
    "    dict_['voiture'] = ['voiture', 'véhicule']\n",
    "    dict_['km'] = ['km', 'kilomètrage', 'kilométrage', 'distance', 'kilomètre']\n",
    "    dict_['essence'] = ['essence', 'carburant', 'énergie']\n",
    "    dict_['roule'] = ['roule', 'roulent', 'marche', 'marcher', 'circule', 'circulent', 'circulation']\n",
    "    dict_['bien'] = ['bien', 'bonne', 'good', 'excellent', 'parfait', 'beau', 'ok']\n",
    "    dict_['min']  = ['min', 'minimum', 'minimal']\n",
    "    dict_['pneu'] = ['pneumatique', 'pneu', 'pneus']\n",
    "    dict_['pouquoi'] = ['cause', 'pourquoi', 'raison', 'motif']\n",
    "    dict_['stupid'] = ['stupid', 'con', 'idiot', 'bête', 'imbécile']\n",
    "    dict_['connaître'] = ['connaître', 'connaitre', 'savoir', 'comprendre',\\\n",
    "                          'connaisance', 'connais', 'connaîs', 'sais', 'savez']\n",
    "    dict_['problème']  = ['problème', 'difficulté', 'danger', 'ennui', 'dangers']\n",
    "    dict_['erreur'] = ['faute', 'erreur']\n",
    "    dict_['arrêt'] = ['arrêt', 'arrêter', 'stopper', 'immobilisé', 'immobiliser', 'immobile', 'paralisé']\n",
    "    dict_['disponible'] = ['disponible', 'disponibilité', 'libre']\n",
    "    dict_['abimer'] = ['abîmer', 'hors service', 'cassé', 'endommager', 'abîmé']\n",
    "    dict_['boitier'] = ['boitier', 'boîtier', 'boîte']\n",
    "    dict_['frein'] = ['frein', 'freinage', 'freiner']\n",
    "    dict_['parcouru'] = ['parcouru', 'parcourus', 'parcourir']\n",
    "    dict_['peux'] = ['peux', 'pouvoir', 'pourrais']\n",
    "    dict_['veux'] = ['veux', 'vouloir', 'veut', 'voulais', 'voudrais', 'souhaiter', 'souhaite']\n",
    "    dict_['fait'] = ['fait', 'fais', 'faire', 'faites']\n",
    "    dict_['fort'] = ['fort', 'puissant', 'robuste', 'solide']\n",
    "    dict_['grand'] = ['grand', 'gross', 'élevé', 'haut', 'haute']\n",
    "    dict_['mal'] = ['mal', 'mauvais', 'souci']\n",
    "    dict_['changer'] = ['modifier', 'modification', 'changer', 'changement']\n",
    "    dict_['avoir'] = ['as', 'obtenir', 'avoir', 'posséder', 'disposer']\n",
    "    dict_['peur']  = ['peur', 'craint']\n",
    "    dict_['placer'] = ['placer','installer']\n",
    "    dict_['nombre'] = ['nombre', 'combien', 'quantité']\n",
    "    dict_['aider']  = ['aider', 'aide', 'soutien', 'soutenir']\n",
    "    dict_['question'] = ['question', 'demande']\n",
    "    dict_['information'] = ['information', 'infos', 'renseignement', 'indictation',\\\n",
    "                            'informer', 'renseigner','indiquer']\n",
    "    dict_['usé'] = ['usé', 'usure', 'vieux', 'vieille', 'fatigué']\n",
    "    dict_['réduire']  = ['réduire', 'réduction', 'abaisser', 'diminuer', 'diminution']\n",
    "    dict_['présent']  = ['présent', 'maitenant', 'actuel', 'actuellement']\n",
    "    dict_['suivre']   = ['suivre', 'observer', 'surveiller', 'poursuivre']\n",
    "    dict_['aller']    = ['aller', 'vas', 'vais', 'va']\n",
    "    dict_['échanger'] = ['échanger', 'estimer', 'échangement', 'estimation',\\\n",
    "                         'évaluer', 'calculer','évaluation'\\\n",
    "                        , 'mesurer']\n",
    "    dict_['taux']    = ['taux', 'pourcentage']\n",
    "    dict_['ampoule'] = ['ampoule', 'feux', 'feu']\n",
    "    for key in dict_:\n",
    "        l = []\n",
    "        for word in dict_[key]:\n",
    "            l.append(stemmer.stem(word))\n",
    "        list_.append(l)\n",
    "    return list_\n",
    "\n",
    "    \n",
    "                \n",
    "def chatting_with_test_corpus():\n",
    "    \n",
    "    path = 'test.txt'\n",
    "    questions = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "                line = line.strip()\n",
    "                questions.append(line)\n",
    "    f.close()\n",
    "    questions_clean = []\n",
    "    for question in questions:\n",
    "        if question not in questions_clean:\n",
    "            questions_clean.append(question)\n",
    "    for line in questions_clean:\n",
    "            line1 = line\n",
    "            time, time_word = _find_right_time(line, COMPLET_DICT_TIME)\n",
    "            list_city = _find_right_geography(line, CITY_LIST)\n",
    "            if time_word is not None:\n",
    "                line = line.replace(time_word, '')\n",
    "            if list_city is not None:\n",
    "                for city in list_city:\n",
    "                    line = line.replace(city, '')\n",
    "            LINE = clear_line(line, STOPWORDS)\n",
    "            unknown_word =0\n",
    "            print('Vous: ', line1)\n",
    "            for word in LINE.split():\n",
    "                if word not in LIST_OF_WORDS:\n",
    "                    unknown_word += 1\n",
    "            if unknown_word > 1:\n",
    "                print('Bot: Désolé, je ne comprends pas ta question, peux tu la reformuler s\\'il te plaît?')\n",
    "                print()\n",
    "                continue\n",
    "            if len(line) !=0:\n",
    "                index= find_right_index(LINE, DICT, méthode)\n",
    "                if time == None and list_city == None:\n",
    "                     print('Bot: ', pairs_trains[index][1])\n",
    "                elif list_city ==None:\n",
    "                    print('Bot: {}, {}'.format(time_word, pairs_trains[index][1]))\n",
    "                elif time == None:\n",
    "                    cities = ' '.join(list_city)\n",
    "                    print('Bot: À {}, {}'.format(cities, pairs_trains[index][1]))\n",
    "                else:\n",
    "                    cities = ' '.join(list_city)\n",
    "                    print('Bot: À {}, {}, {}'.format(cities, time_word, pairs_trains[index][1]))\n",
    "               \n",
    "                LINE = clear_line(line, STOPWORDS)   \n",
    "                index = find_right_index(LINE, DICT, METHODE[0])\n",
    "                #print('YOU: ', line)\n",
    "                #print('BOT: ', pairs_trains[index][1])\n",
    "                print()\n",
    "            \n",
    "\n",
    "def get_all_and_all_convos():\n",
    "    convos = []\n",
    "    file1 = 'data/convos27juin_test.txt'\n",
    "    file2 = 'data/convos27juin_train.txt'\n",
    "    files = [file1, file2]\n",
    "    for file in files:\n",
    "        with open(file, 'r') as f:\n",
    "            i=0\n",
    "            for line in f:\n",
    "                if i%2==0:\n",
    "                    question = line.strip()\n",
    "                    if '++++' in question:\n",
    "                         question = question[9:]\n",
    "                else:\n",
    "                    answer = line.strip()\n",
    "                    if '++++' in answer:\n",
    "                        answer = answer[9:]\n",
    "                    convos.append([question, answer])\n",
    "                i+=1\n",
    "        f.close()\n",
    "    return convos\n",
    "\n",
    "\n",
    "def get_all_convos_in_movies():\n",
    "    convos = []\n",
    "    raw_questions = []\n",
    "    raw_convos = []\n",
    "    for root, dirs, files in os.walk(\"soustitre\"):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                path = os.path.join(root, file)\n",
    "                i = 0\n",
    "                with open(path, 'r') as f:\n",
    "                    get_ = False\n",
    "                    for line in f.readlines():\n",
    "                        line = line.strip('-')\n",
    "                        if i > 3:\n",
    "                            if '?' in line and get_ == False:\n",
    "                                question = line.strip()\n",
    "                                get_ = True\n",
    "                            else:\n",
    "                                if get_ == True:\n",
    "                                    answer = line.strip()\n",
    "                                    convos.append([clear_line(question), answer])\n",
    "                                    raw_convos.append([question, answer])\n",
    "                                    raw_questions.append(question)\n",
    "                                    get_ = False\n",
    "                          \n",
    "                        i +=1\n",
    "    index_of_train = random.sample(range(len(convos)), int(.8*len(convos)))\n",
    "    raw_questions = [raw_questions[i] for i in index_of_train]\n",
    "    pairs_trains = [convos[i] for i in index_of_train]\n",
    "    pairs_tests = [convos[i] for i in range(len(convos)) if i not in index_of_train]\n",
    "    return pairs_trains, pairs_tests, raw_questions, raw_convos\n",
    "\n",
    "def clear_all_convos(all_convos, name):\n",
    "    \"\"\"\n",
    "    Supprimer tous les couples où les questions sont les mêmes\"\"\"\n",
    "    index_of_convos = []\n",
    "    question = []\n",
    "    index =0\n",
    "    for pair in all_convos:\n",
    "        if pair[0] not in question:\n",
    "            index_of_convos.append(index)\n",
    "            question.append(pair[0])\n",
    "        index +=1\n",
    "    convos = [all_convos[index] for index in index_of_convos]\n",
    "    print('Il y a {} de couples de  {}'.format(len(convos), name))\n",
    "    return convos\n",
    "\n",
    "def save_convos_in_file(all_convos):\n",
    "    \n",
    "    index_of_train = random.sample(range(len(all_convos)), int(len(all_convos)*0.80))\n",
    "    with open('convos6juillet_train.txt', 'w') as f:\n",
    "        for index in index_of_train:\n",
    "            pair = all_convos[index]\n",
    "            f.write('YOU ++++ '+ pair[0]+'\\n')\n",
    "            f.write('BOT ++++ '+ pair[1]+'\\n')\n",
    "    f.close()\n",
    "    with open('convos6juillet_test.txt', 'w') as f:\n",
    "        for index in range(len(all_convos)):\n",
    "            if index not in index_of_train:\n",
    "                pair = all_convos[index]\n",
    "                f.write('YOU ++++ '+ pair[0]+'\\n')\n",
    "                f.write('BOT ++++ '+ pair[1]+'\\n')\n",
    "    f.close()\n",
    "    len_test = len(all_convos)- len(index_of_train)\n",
    "    print('save {} convos in train and {} convos in test'.format(len(index_of_train), len_test))\n",
    "    \n",
    "def test_to_find_error(dict):\n",
    "    \n",
    "    index_of_error_prediction = []\n",
    "    index_of_true_pair = []\n",
    "    i = 0\n",
    "    for pair in pairs_trains:\n",
    "        index = find_right_index(pair[0],dict,  'normal')\n",
    "        if index != i:\n",
    "            index_of_error_prediction.append(index)\n",
    "            index_of_true_pair.append(i)\n",
    "        i +=1\n",
    "    for i in range(len(index_of_error_prediction)):\n",
    "        index1 = index_of_error_prediction[i]\n",
    "        index2 = index_of_true_pair[i]\n",
    "        score1 = score_by_new_doc(pairs_trains[index1][0], pairs_trains[index2][0],dict, 'normal')\n",
    "        score2 = score_by_new_doc(pairs_trains[index2][0], pairs_trains[index2][0],dict, 'normal')\n",
    "        print('False return question--- {} ---with score {}'.format(pairs_trains[index1][0], score1))\n",
    "        print('True return question--- {} ---with score {}'.format(pairs_trains[index2][0], score2))\n",
    "\n",
    "def print_all_errors_on_test(dict):\n",
    "    for pair in pairs_tests:\n",
    "        question = clear_line(pair[0], STOPWORDS)\n",
    "        index = find_right_index(question, dict, 'normal')\n",
    "        print(pair)\n",
    "        print(pairs_trains[index])\n",
    "        print()\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def mean_score(doc, dict_):\n",
    "    if len(doc) == 0:\n",
    "        return 0\n",
    "    total_score = 0\n",
    "    doc_ =  doc.lower().strip().strip('?').strip('.').strip('!').split()\n",
    "    for pharse in dict_:\n",
    "        score = 0\n",
    "        for word in doc_:\n",
    "            if word in dict_[pharse]:\n",
    "                score +=dict_[pharse][word]\n",
    "        total_score += score/len(doc_)\n",
    "    return total_score/len(dict_)\n",
    "\n",
    "def find_right_index_for_classif(q, questions,  dict_):\n",
    "\n",
    "    index=0\n",
    "    biggest_score = score_by_new_doc_(q, questions[0], dict_)\n",
    "    for i in range(len(questions)):\n",
    "        boolean1 = score_by_new_doc_(q, questions[i], dict_) >  biggest_score\n",
    "        boolean2 = score_by_new_doc_(q, questions[i], dict_) == biggest_score\n",
    "        boolean3 = len(questions[index]) >len(questions[i])\n",
    "        if boolean1 or (boolean2 and boolean3):\n",
    "            biggest_score = score_by_new_doc_(q, questions[i], dict_)\n",
    "            index = i\n",
    "    return index\n",
    "    \n",
    "    \n",
    "\n",
    "def classification(question, dict_sujet_, dict_hors_):\n",
    "    \n",
    "    seuil = 0.75\n",
    "    Q1, Q2 = get_questions_for_classification()\n",
    "    index_sujet = find_right_index_for_classif(question, Q1, dict_sujet_)\n",
    "    index_hors  = find_right_index_for_classif(question, Q2, dict_hors_)\n",
    "    score_sujet = score_by_new_doc_(question, Q1[index_sujet], dict_sujet_)\n",
    "    score_hors  = score_by_new_doc_(question, Q2[index_hors], dict_hors_)\n",
    "    test = score_sujet > score_hors\n",
    "    précision_sujet, rappel_sujet = precision_recall(question, Q1[index_sujet])\n",
    "    précision_hors, rappel_hors = precision_recall(question, Q2[index_hors])\n",
    "    if test and précision_sujet >= seuil and rappel_sujet >= seuil:\n",
    "        return True\n",
    "    elif not test and précision_hors >= seuil and rappel_hors >= seuil:\n",
    "        return False\n",
    "    else:\n",
    "        if mean_score(question, dict_sujet_) > mean_score(question, dict_hors_):\n",
    "            #return False\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    \n",
    "def test_classification_by_corpus(dict_sujet, dict_hors):\n",
    "    \n",
    "\n",
    "    path = 'processed/chat_tfidf.txt'\n",
    "    i = 0\n",
    "    questions = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            if i%2 ==0:\n",
    "                if '++++' in line:\n",
    "                    line = line[8:]\n",
    "                line = line.strip('+').strip()\n",
    "                questions.append(line)\n",
    "            i +=1\n",
    "    f.close()\n",
    "    total_value = 0\n",
    "    for question in questions:\n",
    "        value = classification(question, dict_sujet, dict_hors)\n",
    "        total_value +=value\n",
    "        print(question,'---------',value)\n",
    "    print('Le pourcentage des sujets sont {}'.format(total_value/len(questions)))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def question_processing(question):\n",
    "    question_ = question.strip().split()\n",
    "    if len(question_) == 1:\n",
    "        return question\n",
    "    else:\n",
    "        index = random.sample(range(len(question_)), len(question_)-1)\n",
    "    return ' '.join([question_[i] for i in sorted(index)])\n",
    "\n",
    "\n",
    "def dict_of_all_score_(convos, KEY_LIST, sujet):\n",
    "    \"\"\"\n",
    "    return score of all word in all doc\"\"\"\n",
    "    DICT = {}\n",
    "    for doc_ in convos:\n",
    "        doc = doc_[0]\n",
    "        DICT[doc]=dict_doc_score_(doc,KEY_LIST, sujet)\n",
    "    return DICT \n",
    "\n",
    "\n",
    "\n",
    "def find_right_index_(new_doc, sujet):\n",
    "    \n",
    "    if sujet:\n",
    "        DICT = dict_sujet\n",
    "        convos = convos_sujet\n",
    "    else:\n",
    "        DICT = dict_hors\n",
    "        convos = convos_hors\n",
    "    index=0\n",
    "    biggest_score = score_by_new_doc_(new_doc, convos[0][0],DICT)\n",
    "    for i in range(len(convos)):\n",
    "        boolean1 = score_by_new_doc_(new_doc, convos[i][0], DICT) > biggest_score\n",
    "        boolean2 = score_by_new_doc_(new_doc, convos[i][0], DICT) == biggest_score\n",
    "        boolean3 = len(convos[index][0]) >len(convos[i][0])\n",
    "        if boolean1 or (boolean2 and boolean3):\n",
    "            biggest_score = score_by_new_doc_(new_doc, convos[i][0], DICT)\n",
    "            index = i\n",
    "    return index\n",
    "\n",
    "\n",
    "def tf_idf_(word, doc, KEY_LIST, sujet):\n",
    "    \n",
    "    if sujet:\n",
    "        convos = convos_sujet\n",
    "    else:\n",
    "        convos = convos_hors\n",
    "    \n",
    "    tf, df = 0, 0\n",
    "    words = split_words(doc)\n",
    "    #len_doc = len(words)\n",
    "    #value_doc = 1/math.sqrt(len_doc)\n",
    "    #value_doc = math.sqrt(value_doc)\n",
    "    if word in KEY_LIST:\n",
    "        value_word = 10\n",
    "    else:\n",
    "        value_word = 1\n",
    "    if word in str(doc):\n",
    "        tf = value_word\n",
    "    for doc_ in convos:\n",
    "        if word in str(doc_[0]):\n",
    "            df+=1\n",
    "    if df>0:\n",
    "            return tf*math.log(len(convos)/(df), 10)\n",
    "    else:\n",
    "            return 0\n",
    "        \n",
    "def tf_idf_for_classif(word, doc, sujet = True):\n",
    "    \n",
    "    convos1, convos2 = get_questions_for_classification()\n",
    "    if sujet:\n",
    "        convos = convos1\n",
    "    else:\n",
    "        convos = convos2\n",
    "    \n",
    "    tf, df = 0, 0\n",
    "    if word in str(doc):\n",
    "        tf = 1\n",
    "    for doc_ in convos:\n",
    "        if word in str(doc_):\n",
    "            df+=1\n",
    "    if df>0:\n",
    "            return tf*math.log(len(convos)/(df), 10)\n",
    "    else:\n",
    "            return 0\n",
    "\n",
    "def chat_with_each_subjet(raw_question, sujet = True):\n",
    "    \n",
    "    if sujet:\n",
    "        DICT = dict_sujet\n",
    "        convos = convos_sujet\n",
    "    else:\n",
    "        DICT = dict_hors\n",
    "        convos = convos_hors\n",
    "    list_city = _find_right_geography(raw_question, CITY_LIST)\n",
    "    line = raw_question.lower().strip().strip('?').strip('.')\n",
    "    time, time_word = _find_right_time(line, COMPLET_DICT_TIME)\n",
    "    if time_word is not None:\n",
    "                line = line.replace(time_word, '')\n",
    "    if list_city is not None:\n",
    "                for city in list_city:\n",
    "                    line = line.replace(city, '')\n",
    "    LINE = clear_line(line, STOPWORDS)\n",
    "    if len(line) !=0:\n",
    "            index= find_right_index_(LINE, sujet)\n",
    "            good_question = convos[index][0]\n",
    "           # if not reformule_question_if_need(LINE, good_question):\n",
    "                   # get_answer = str(input('Tu veux demander: {}?  |'.format(questions_to_print[index])))\n",
    "                   # if 'non' in get_answer.lower():\n",
    "                    #    print('Peux tu reformuler ta question s\\'il te plaît?')\n",
    "                     #   continue\n",
    "            if time == None and list_city == None:\n",
    "                     answer= convos[index][1]\n",
    "            elif list_city ==None:\n",
    "                    answer = time_word + ' '+ convos[index][1]\n",
    "            elif time == None:\n",
    "                    cities = ' '.join(list_city)\n",
    "                    answer = 'À ' + cities +' '+ convos[index][1]\n",
    "            else:\n",
    "                    cities = ' '.join(list_city)\n",
    "                    answer = 'À ' + cities +' '+ time_word+ ' '+ convos[index][1]\n",
    "            return answer, index, line\n",
    "    else:\n",
    "        return None, None, line\n",
    "    \n",
    "\n",
    "def request_missing_infos(new_question, good_question, dict_):\n",
    "    \n",
    "    if 'to_return' in good_question:\n",
    "        return True\n",
    "    score1 = score_by_new_doc_(new_question, good_question, dict_)\n",
    "    score2 = 0\n",
    "    for word in dict_[good_question]:\n",
    "        score2 += dict_[good_question][word]\n",
    "    if score1/score2 > 0.6:\n",
    "        return  True\n",
    "    else:\n",
    "        return  False\n",
    "    \n",
    "def to_print_answer(line):\n",
    "    line = str(line)\n",
    "    first = line[0]\n",
    "    line = first.upper()+ line[1:]\n",
    "    if line[-1] not in ['?','.','!']:\n",
    "        line = line+'.'\n",
    "    line = line.strip(' ')\n",
    "    return line\n",
    "    \n",
    "def chat_():\n",
    "    \n",
    "    print('This will be LDA method....')\n",
    "    print('Bonjour, le bot d\\'Avicen à ton écoute')\n",
    "    path = 'processed/chat_tfidf.txt'\n",
    "    f =  open(path, 'a+') \n",
    "    while True:\n",
    "            raw_question = str(input('You: '))\n",
    "            f.write('YOU++++ '+ raw_question + '\\n')\n",
    "            if len(raw_question) < 2:\n",
    "                break\n",
    "            question = clear_line(raw_question, STOPWORDS)\n",
    "            question = split_words(question)\n",
    "            new_bow = dictionary.doc2bow(question)\n",
    "            new_doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=new_bow)])\n",
    "            most_sim_ids = get_most_similar_documents(new_doc_distribution,doc_topic_dist)\n",
    "            index  = most_sim_ids[0]\n",
    "            answer = convos_train[index][1]\n",
    "            print('Bot: ', answer)\n",
    "            \n",
    "    \n",
    "def dict_doc_score_(doc, KEY_LIST, sujet):\n",
    "    \n",
    "    \"\"\"\n",
    "    We return score of all one words and all two consequent words\"\"\"\n",
    "\n",
    "    list_word_ = split_words(doc)    \n",
    "    return {word: tf_idf_(word, doc, KEY_LIST, sujet) for word in list_word_}\n",
    "   \n",
    "\n",
    "def dict_doc_score_for_classif(doc, sujet): \n",
    "    \"\"\"\n",
    "    We return score of all one words and all two consequent words\"\"\"\n",
    "\n",
    "    list_word_ = split_words(doc)    \n",
    "    return {word: tf_idf_for_classif(word, doc, sujet) for word in list_word_}\n",
    "    \n",
    "def score_by_new_doc_(new_doc, doc, DICT):\n",
    "    \n",
    "    list_word_in_doc = split_words(new_doc)\n",
    "    score = 0\n",
    "    for word in list_word_in_doc:\n",
    "            if word in DICT[doc]:\n",
    "                 score += DICT[doc][word]\n",
    "    return score\n",
    "\n",
    "\n",
    "\n",
    "def train_lda(data, num_topics, chunksize, alpha, eta):\n",
    "    \"\"\"\n",
    "    This function trains the lda model\n",
    "    We setup parameters like number of topics, the chunksize to use in Hoffman method\n",
    "    We also do 2 passes of the data since this is a small dataset, so we want the distributions to stabilize\n",
    "    \"\"\"\n",
    "    dictionary = corpora.Dictionary(data)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in data]\n",
    "    t1 = time.time()\n",
    "    # low alpha means each document is only represented by a small number of topics, and vice versa\n",
    "    # low eta means each topic is only represented by a small number of words, and vice versa\n",
    "    lda = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary,\n",
    "                   alpha=alpha, eta=eta, chunksize=chunksize, minimum_probability=0.0, passes=2)\n",
    "    t2 = time.time()\n",
    "   # print(\"Time to train LDA model on \", len(data), \"articles: \", int(t2-t1), \"seconds\")\n",
    "    return dictionary, corpus, lda\n",
    "\n",
    "def find_best_model(data):\n",
    "    start = time.time()\n",
    "    num_topics_best, chunk_best, alpha_best, eta_best = 5, 1, 1e-2, 0.5e-2\n",
    "    dictionary, corpus, lda = train_lda(data, num_topics_best, chunk_best, alpha_best, eta_best)\n",
    "    precision_best, recall_best, F_best = calcul_precision_recall_(train = True)\n",
    "    num_topics = [5, 10, 20, 30, 60, 120, 240 ]\n",
    "    chunksize = [128]\n",
    "    alpha = [1e-2, 1e-1, 1, 1e+1]\n",
    "    eta = [0.5e-2, 1e-2, 1e-1, 1, 1e+1]\n",
    "    for num_topic in num_topics:\n",
    "        for chunk in chunksize:\n",
    "            for alpha_ in alpha:\n",
    "                for eta_ in eta:\n",
    "                    dictionary, corpus, lda = train_lda(data, num_topic, chunk, alpha_, eta_)\n",
    "                    precision, recall, F = calcul_precision_recall_(train = True)\n",
    "                    if F > F_best:\n",
    "                        num_topic_best = num_topic\n",
    "                        chunk_best = chunk\n",
    "                        alpha_best = alpha_\n",
    "                        eta_best = eta_\n",
    "                        precision_best = precision\n",
    "                        recall_best = recall\n",
    "                        F_best = F\n",
    "    print('Best result is precision {}, recall {} and F score {}'.format(\\\n",
    "                                precision_best, recall_best, F_best))\n",
    "    print('Best parameters are num_topic {} chunk {}, alpha {} et eta {}'.format(\\\n",
    "                    num_topic_best, chunk_best, alpha_best, eta_best))\n",
    "    print('Time ', time.time()-start)\n",
    "                    \n",
    "    \n",
    "def get_all_words(convos):\n",
    "    \"\"\"\n",
    "    prend tous les conversations dans corpus et renvoie un array de documents avec\n",
    "    des vocabulaires séparés\"\"\"\n",
    "    \n",
    "    data = []\n",
    "    for convo in convos:\n",
    "        data.append(split_words(convo[0]))\n",
    "    return data\n",
    "\n",
    "def jensen_shannon(query, matrix):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function implements a Jensen-Shannon similarity\n",
    "    between the input query (an LDA topic distribution for a document)\n",
    "    and the entire corpus of topic distributions.\n",
    "    It returns an array of length M where M is the number of documents in the corpus\n",
    "    \"\"\"\n",
    "    # lets keep with the p,q notation above\n",
    "    p = query[None,:].T # take transpose\n",
    "    q = matrix.T # transpose matrix\n",
    "    m = 0.5*(p + q)\n",
    "    return np.sqrt(0.5*(entropy(p,m) + entropy(q,m)))\n",
    "\n",
    "def get_most_similar_documents(query, matrix, k=10):\n",
    "    \"\"\"\n",
    "    This function implements the Jensen-Shannon distance above\n",
    "    and returns the top k indices of the smallest jensen shannon distances\n",
    "    \"\"\"\n",
    "    sims = jensen_shannon(query,matrix) # list of jensen shannon distances\n",
    "    return sims.argsort()[:k] # the top k positional index of the smallest Jensen Shannon distan\n",
    "\n",
    "\n",
    "def calcul_precision_recall_(train = False): \n",
    "    \n",
    "    if not train:\n",
    "        total_precision, total_recall = 0, 0\n",
    "        len_ = len(convos_test)\n",
    "        index_to_print = random.sample(range(len_), 30)\n",
    "        for i in range(len(convos_test)):\n",
    "            question = convos_test[i][0]\n",
    "            question = split_words(question)\n",
    "            new_bow = dictionary.doc2bow(question)\n",
    "            new_doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=new_bow)])\n",
    "            most_sim_ids = get_most_similar_documents(new_doc_distribution, doc_topic_dist)\n",
    "            index = most_sim_ids[0]\n",
    "            answer = convos_train[index][1]\n",
    "            good_answer = convos_test[i][1]\n",
    "            precision, recall = precision_recall\\\n",
    "            (answer, good_answer)\n",
    "            total_precision  += precision\n",
    "            total_recall     += recall\n",
    "        total_precision = total_precision/len_\n",
    "        total_recall = total_recall/len_      \n",
    "        F = 2*total_precision*total_recall/(total_precision+total_recall)\n",
    "       # print('ON TEST, WE HAVE: PRECISION = {:.5f},  RECALL = {:.5f}, F_MESURE = {:.3f} '\\\n",
    "        #      .format(total_precision, total_recall,F))\n",
    "        #print() \n",
    "    else:\n",
    "        total_precision, total_recall = 0, 0\n",
    "        len_ = len(convos_train)\n",
    "        index_to_print = random.sample(range(len_), 30)\n",
    "        for i in range(len(convos_train)):\n",
    "            question = convos_train[i][0]\n",
    "            question = split_words(question)\n",
    "            new_bow = dictionary.doc2bow(question)\n",
    "            new_doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=new_bow)])\n",
    "            most_sim_ids = get_most_similar_documents(new_doc_distribution,doc_topic_dist)\n",
    "            index = most_sim_ids[0]\n",
    "            answer = convos_train[index][1]\n",
    "            good_answer = convos_train[i][1]\n",
    "            precision, recall = precision_recall\\\n",
    "            (answer, good_answer)\n",
    "            total_precision  += precision\n",
    "            total_recall     += recall\n",
    "        total_precision = total_precision/len_\n",
    "        total_recall = total_recall/len_      \n",
    "        F = 2*total_precision*total_recall/(total_precision+total_recall)\n",
    "      #  print('ON TRAIN, WE HAVE: PRECISION = {:.5f},  RECALL = {:.5f}, F_MESURE = {:.3f} '\\\n",
    "       #       .format(total_precision, total_recall,F))\n",
    "       # print()\n",
    "    return total_precision, total_recall, F\n",
    "\n",
    "def data_processed():\n",
    "    \"\"\"\n",
    "    Traiter et retourner les conversations avec les questions bien nettoyées\n",
    "    les questions sans nettoyer et le couple pour test\n",
    "    \"\"\"\n",
    "    convos1, questions1 = get_all_convos(STOPWORDS,'data/trains_sujet.txt')\n",
    "    convos2, questions2  = get_all_convos(STOPWORDS, 'data/trains_hors_sujet.txt')\n",
    "    convos3, questions3 = get_all_convos(STOPWORDS, 'data/convos3septembre_train.txt')\n",
    "    convos1.extend(convos2)\n",
    "    convos1.extend(convos3)\n",
    "    questions1.extend(questions2)\n",
    "    questions1.extend(questions3)\n",
    "    convos_test1, questions_test1 = get_all_convos(STOPWORDS, 'data/tests_16juillet.txt')\n",
    "    convos_test2, questions_test2 = get_all_convos(STOPWORDS, 'data/convos3septembre_test.txt')\n",
    "    convos_test1.extend(convos_test2)\n",
    "    questions_test1.extend(questions_test2)\n",
    "    index_suff = [i for i in range(len(convos1))]\n",
    "    random.shuffle(index_suff)\n",
    "    convos, questions = [], []\n",
    "    for i in index_suff:\n",
    "        convos.append(convos1[i])\n",
    "        questions.append(questions1[i])\n",
    "    index_suff_ = [i for i in range(len(convos_test1))]\n",
    "    random.shuffle(index_suff_)\n",
    "    convos_test, questions_test = [], []\n",
    "    for i in index_suff_:\n",
    "        convos_test.append(convos_test1[i])\n",
    "        questions_test.append(questions_test1[i])    \n",
    "    return convos, questions, convos_test, questions_test\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    start = time.time()\n",
    "    try:\n",
    "        convos_train_\n",
    "    except NameError:\n",
    "        convos_train, questions, convos_test, questions_test = data_processed()\n",
    "        data = get_all_words(convos_train)\n",
    "        start = time.time()\n",
    "        dictionary, corpus, lda = train_lda(data, 240, 128, 0.1, 0.005)\n",
    "        doc_topic_dist = np.array([[tup[1] for tup in lst] for lst in lda[corpus]])\n",
    "        pr, recall, F = calcul_precision_recall_(train = False)\n",
    "        print('Training on {} couples and test on {} couples'.\\\n",
    "             format(len(convos_train), len(convos_test)))\n",
    "        print('on test, nous avons les scores sont pre {:.5f} recall {:.5f} et F {:.5f}'.format(pr, recall, F))\n",
    "        pr, recall, F = calcul_precision_recall_(train = True)\n",
    "        print('on train, nous avons, precision {:.5f} recall {:.5f} et F {:.5f}'.format(pr, recall, F))\n",
    "        print('all time is ', int(time.time()-start), ' seconds')\n",
    "        #Best parameters are num_topic 240 chunk 128, alpha 0.1 et eta 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modifi chaqu comet ----- On en change pas à chaque comète \n",
      "quel nombr voitur disponibl ----- quel est le nombre de véhicules disponibles\n",
      "ailleur ----- Qui c'est, d'ailleurs \n",
      "jour tabl rond ----- C'est jour de table ronde \n",
      "pouv aid ----- vous pouvez m'aider\n",
      "manqu truc ----- Il manque pas un truc \n",
      "sembl as problem km ----- il semble avoir un problème avec le kilométrage\n",
      "quel voitur vitess plus faibl ----- quels véhicules ont vitesse plus faible\n",
      "où où ----- D'où venez-vous ? Où allez-vous \n",
      "quel consomm moyen voitur ----- quelle est la consommation moyenne de mon véhicule\n"
     ]
    }
   ],
   "source": [
    "for j in range(10):\n",
    "    i = random.choice([i for i in range(100)])\n",
    "    print(convos_test[i][0], '-----',questions_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/gensim/models/ldamodel.py:775: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    }
   ],
   "source": [
    "dictionary, corpus, lda = train_lda(data, 240, 128, 0.1, 0.005)\n",
    "doc_topic_dist = np.array([[tup[1] for tup in lst] for lst in lda[corpus]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombr voitur progressent rout\n",
      "['nombr', 'voitur', 'progressent', 'rout', 'nombr voitur', 'voitur progressent', 'progressent rout', 'nombr voitur progressent', 'voitur progressent rout']\n",
      "[(71, 1), (257, 1), (258, 1), (681, 1), (5319, 1), (5320, 1), (5321, 1), (5322, 1), (5323, 1)]\n"
     ]
    }
   ],
   "source": [
    "convo = random.choice(convos_train)\n",
    "question = convo[0]\n",
    "print(question)\n",
    "question = split_words(question)\n",
    "print(question)\n",
    "new_bow = dictionary.doc2bow(question)\n",
    "print(new_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_lda' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4720f4a30f55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_lda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m240\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlst\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlst\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalcul_precision_recall_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_lda' is not defined"
     ]
    }
   ],
   "source": [
    "dictionary, corpus, lda = train_lda(data, 240, 128, 0.1, 0.005)\n",
    "doc_topic_dist = np.array([[tup[1] for tup in lst] for lst in lda[corpus]])\n",
    "def calcul_precision_recall_(train = False): \n",
    "    \n",
    "    if not train:\n",
    "        total_precision, total_recall = 0, 0\n",
    "        len_ = len(convos_test)\n",
    "        index_to_print = random.sample(range(len_), 30)\n",
    "        for i in range(len(convos_test)):\n",
    "            question = convos_test[i][0]\n",
    "            question = split_words(question)\n",
    "            new_bow = dictionary.doc2bow(question)\n",
    "            new_doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=new_bow)])\n",
    "            most_sim_ids = get_most_similar_documents(new_doc_distribution, doc_topic_dist)\n",
    "            index = most_sim_ids[0]\n",
    "            answer = convos_train[index][1]\n",
    "            good_answer = convos_test[i][1]\n",
    "            precision, recall = precision_recall\\\n",
    "            (answer, good_answer)\n",
    "            total_precision  += precision\n",
    "            total_recall     += recall\n",
    "        total_precision = total_precision/len_\n",
    "        total_recall = total_recall/len_      \n",
    "        F = 2*total_precision*total_recall/(total_precision+total_recall)\n",
    "        print('ON TEST, WE HAVE: PRECISION = {:.5f},  RECALL = {:.5f}, F_MESURE = {:.3f} '\\\n",
    "              .format(total_precision, total_recall,F))\n",
    "        print() \n",
    "    else:\n",
    "        total_precision, total_recall = 0, 0\n",
    "        len_ = len(convos_train)\n",
    "        index_to_print = random.sample(range(len_), 30)\n",
    "        for i in range(len(convos_train)):\n",
    "            question = convos_train[i][0]\n",
    "            question = split_words(question)\n",
    "            new_bow = dictionary.doc2bow(question)\n",
    "            new_doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=new_bow)])\n",
    "            most_sim_ids = get_most_similar_documents(new_doc_distribution, doc_topic_dist)\n",
    "            index = most_sim_ids[0]\n",
    "            answer = convos_train[index][0]\n",
    "            good_answer = convos_train[i][0]\n",
    "            precision, recall = precision_recall\\\n",
    "            (answer, good_answer)\n",
    "            total_precision  += precision\n",
    "            total_recall     += recall\n",
    "        total_precision = total_precision/len_\n",
    "        total_recall = total_recall/len_      \n",
    "        F = 2*total_precision*total_recall/(total_precision+total_recall)\n",
    "        print('ON TRAIN, WE HAVE: PRECISION = {:.5f},  RECALL = {:.5f}, F_MESURE = {:.3f} '\\\n",
    "              .format(total_precision, total_recall,F))\n",
    "        print()\n",
    "    return total_precision, total_recall, F\n",
    "calcul_precision_recall_(train = True)\n",
    "calcul_precision_recall_(train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/gensim/models/ldamodel.py:775: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/gensim/models/ldamodel.py:582: RuntimeWarning: overflow encountered in exp2\n",
      "  perwordbound, np.exp2(-perwordbound), len(chunk), corpus_words\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1298: RuntimeWarning: invalid value encountered in sqrt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on test, nous avons, precision 0.47672 recall 0.57861 et F 0.52275\n",
      "on train, nous avons, precision 0.47173 recall 0.55623 et F 0.51051\n"
     ]
    }
   ],
   "source": [
    "def print_best_result():\n",
    "        dictionary, corpus, lda = train_lda(data,700, 1000, 0.1, 0.0001)\n",
    "        doc_topic_dist = np.array([[tup[1] for tup in lst] for lst in lda[corpus]])\n",
    "        pr, recall, F = calcul_precision_recall_(train = False)\n",
    "        print('on test, nous avons, precision {:.5f} recall {:.5f} et F {:.5f}'.format(pr, recall, F))\n",
    "        pr, recall, F = calcul_precision_recall_(train = True)\n",
    "        print('on train, nous avons, precision {:.5f} recall {:.5f} et F {:.5f}'.format(pr, recall, F))\n",
    "        #print('all time is ', int(time.time()-start), ' seconds')\n",
    "print_best_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2096\n",
      "[0.01666665 0.0166666  0.01666674 0.01666714 0.01666662 0.01666693\n",
      " 0.01666654 0.01666683 0.01666686 0.01666637 0.01666689 0.01666704\n",
      " 0.01666643 0.01666656 0.01666632 0.01666691 0.01666651 0.01666643\n",
      " 0.01666642 0.01666675 0.01666659 0.01666681 0.01666705 0.01666684\n",
      " 0.0166662  0.01666686 0.01666665 0.01666643 0.0166667  0.01666659\n",
      " 0.01666643 0.01666706 0.01666638 0.01666692 0.01666666 0.01666727\n",
      " 0.01666697 0.01666685 0.016667   0.01666671 0.0166666  0.01666657\n",
      " 0.01666657 0.0166665  0.0166667  0.01666621 0.01666674 0.01666683\n",
      " 0.01666649 0.01666634 0.01666657 0.01666677 0.01666638 0.01666619\n",
      " 0.0166668  0.01666672 0.01666688 0.01666623 0.01666646 0.01666695]\n"
     ]
    }
   ],
   "source": [
    "print(len(doc_topic_dist))\n",
    "print(random.choice(doc_topic_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This will be LDA method....\n",
      "Bonjour, le bot d'Avicen à ton écoute\n",
      "You: hello\n",
      "Bot:  Vous savez bien.\n",
      "You: salut\n",
      "Bot:  Vous savez bien.\n",
      "You: comment ça va\n",
      "Bot:  ça va merci\n",
      "You: je cherche ma voiture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1303: RuntimeWarning: invalid value encountered in sqrt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:  #ask_id# #problème#\n",
      "You: bon, je veux ma voiture\n",
      "Bot:  #ask_id# #position#\n",
      "You: où est ma femme\n",
      "Bot:  Partie faire un tour.\n",
      "You: où sont les enfants\n",
      "Bot:  Non. Bonne nuit.\n",
      "You: à plus tard\n",
      "Bot:  #ask_id# #moteur_coupé#\n",
      "You: à tout à l'heure\n",
      "Bot:  Non, pas ça !\n",
      "You: pourquoi\n",
      "Bot:  Il faudrait que je me décide ?\n",
      "You: je veux un renseignement\n",
      "Bot:  je suis là pour t'aider\n",
      "You: où est la voiture\n",
      "Bot:  ton véhicule est à #position#\n",
      "You: et le roi est où\n",
      "Bot:  J'en sais rien.\n",
      "You: ta femme\n",
      "Bot:  Il ferait beau voir.\n"
     ]
    }
   ],
   "source": [
    "chat_()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
