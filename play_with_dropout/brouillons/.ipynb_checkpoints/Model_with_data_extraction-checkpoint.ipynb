{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cet technique marche beaucoup mieux que dans tensorflow, avec en très peu de temps.\n",
    "Ce modèle fonctionne mieux que l'autre dans tensorflow parce qu'une raison évidente: il utilise méthode teacher forcing\n",
    "Un autre problème avec ce méthode, c'est dropout qui donne une technique plus ou moins bon. Il donne la réponse plus tôt aléatoire pour une question. En cas général, ce la peut être intéressant, mais dans notre cas, il est très important qu'il capture le mot clés (donc, surtout on risque de  supprimer le mot clés, qui rendra une mauvais réponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'anh khong xing la bien xang'\n",
    "b = a.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.HIDDEN_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Reading lines...\n",
      "Read 555 sentence pairs\n",
      "Trimmed to 548 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "questions 594\n",
      "answers 772\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cet technique marche beaucoup mieux que dans tensorflow, avec en très peu de temps. \n",
    "Ce modèle fonctionne mieux que l'autre dans tensorflow parce qu'une raison évidente: \n",
    "il utilise méthode teacher forcing Un autre problème avec ce méthode, \n",
    "c'est dropout qui donne une technique plus ou moins bon. Il donne la \n",
    "réponse plus tôt aléatoire pour une question. En cas général, \n",
    "ce la peut être intéressant, mais dans notre cas, il est très important \n",
    "qu'il capture le mot clés \n",
    "(donc, surtout on risque de supprimer le mot clés, qui rendra une mauvais réponse)\n",
    "modèle est bien entrainé, donc, il ne sert à rien d'entrainer encore.\n",
    "\"\"\"\n",
    "#from IPython.display import display, Markdown\n",
    "#display(Markdown(\"### Pour lancer un chat, il suffit de taper en même temps CTRL et ENTER\"))\n",
    "#display(Markdown(\"### Pour arrêter le mode chat, il suffit de taper ENTER dans votre conversation\"))\n",
    "%matplotlib inline\n",
    "\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import data\n",
    "import config\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "SOS_token  = config.SOS_token\n",
    "EOS_token  = config.EOS_token\n",
    "MAX_LENGTH = config.MAX_LENGTH\n",
    "stopwords  = config.STOPWORDS\n",
    "learning_rate = config.LEARNING_RATE\n",
    "teacher_forcing_ratio = config.TEACHER_FORCING\n",
    "dropout = config.DROPOUT\n",
    "hidden_size = 70\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  \n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "def normalizeString(s):\n",
    "    \"\"\"\n",
    "    Whith a tring s, we make it in lower case, delete \\n if exists at \n",
    "    the end of string, and delete specical case ? . and !\n",
    "    \"\"\"\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"([,.!?\\n])\", r\"\", s)# sumprimer tous les caractères .! et ?\n",
    "    #s = re.sub(r\"[^a-zA-Z0-9.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def readLangs(questions, answers, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "    pairs = []   \n",
    "    encode_file = open(os.path.join(config.PROCESSED_PATH, \"question.txt\"), 'r')\n",
    "    decode_file = open(os.path.join(config.PROCESSED_PATH, \"answer.txt\"), 'r')\n",
    "    encode, decode = encode_file.readline(), decode_file.readline()\n",
    "    while encode and decode:\n",
    "        encode, decode = normalizeString(encode), normalizeString(decode)\n",
    "        pairs.append([encode, decode])\n",
    "        encode, decode = encode_file.readline(), decode_file.readline()\n",
    "\n",
    "    \n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(answers)\n",
    "        output_lang = Lang(questions)\n",
    "    else:\n",
    "        input_lang = Lang(questions)\n",
    "        output_lang = Lang(answers)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "def TrimWords(pairs):\n",
    "    for pair in pairs: #[pair for pair in pairs]:\n",
    "        resultwords  = [word for word in pair[0].split() if word.lower() not in stopwords]\n",
    "        pair[0] = ' '.join(resultwords)\n",
    "    return pairs\n",
    "\n",
    " \n",
    "def TrimWordsTest(question):\n",
    "    resultwords  = [word for word in question.split() if word.lower() not in stopwords]\n",
    "    question = ' '.join(resultwords)\n",
    "    return question\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH \n",
    "\n",
    "    \n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = TrimWords(pairs)\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        for i in range(self.n_layers):\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        for i in range(self.n_layers):\n",
    "            output = F.relu(output)\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=dropout, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        #embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)))#, dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            output = F.relu(output)\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]))#, dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n",
    "def closetWord(word, lang):\n",
    "    \"\"\"\n",
    "    find and return the closest word in lang\n",
    "    \"\"\"\n",
    "    Dict = lang.word2index\n",
    "    corpus = lang.index2word\n",
    "    if word in Dict:\n",
    "        return word\n",
    "    else:\n",
    "        distance = levenshtein(word, corpus[0])\n",
    "        close_word = corpus[0]\n",
    "        for ix in corpus:\n",
    "            if levenshtein(word, corpus[ix]) <distance:\n",
    "                close_word = corpus[ix]\n",
    "                distance = levenshtein(word, corpus[ix])\n",
    "        if distance <=2:\n",
    "            return close_word\n",
    "        else:\n",
    "            return ''\n",
    "def normalizeSentenceInChat(sentence):\n",
    "    sentence = sentence.strip().lower().split()\n",
    "    s = [closetWord(word, input_lang) for word in sentence]\n",
    "    return ' '.join(s)\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    #sentence = normalizeSentence\n",
    "    return [lang.word2index[word] for word in sentence.split(' ') if word in lang.word2index ]\n",
    "\n",
    "\n",
    "def variableFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    result = Variable(torch.LongTensor(indexes).view(-1, 1))\n",
    "    if use_cuda:\n",
    "        return result.cuda()\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "\n",
    "def variablesFromPair(pair):\n",
    "    input_variable = variableFromSentence(input_lang, pair[0])\n",
    "    target_variable = variableFromSentence(output_lang, pair[1])\n",
    "    return (input_variable, target_variable)\n",
    "\n",
    "\n",
    "def train(input_variable, target_variable, encoder, decoder, \n",
    "          encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_variable.size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "\n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_variable[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0][0]\n",
    "\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]]))\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "            decoder_input = target_variable[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            ni = topi[0][0]\n",
    "\n",
    "            decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "            if ni == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / target_length\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "def trainIters(n_iters, print_every=1000, plot_every=100, learning_rate=learning_rate):\n",
    "    encoder, decoder = restore_model()\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [variablesFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_variable = training_pair[0]\n",
    "        target_variable = training_pair[1]\n",
    "\n",
    "        loss = train(input_variable, target_variable, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)\n",
    "    save_model(encoder, decoder)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    sentence = normalizeSentenceInChat(sentence)\n",
    "    input_variable = variableFromSentence(input_lang, sentence)\n",
    "    input_length = input_variable.size()[0]\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_variable[ei],\n",
    "                                                 encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_outputs[ei] + encoder_output[0][0]\n",
    "\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]]))  \n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs)\n",
    "        decoder_attentions[di] = decoder_attention.data\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        ni = int(ni)\n",
    "        if ni == EOS_token:\n",
    "            decoded_words.append('<EOS>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(output_lang.index2word[ni])\n",
    "\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    return decoded_words, decoder_attentions[:di + 1]\n",
    "\n",
    "def evaluateRandomly(n):\n",
    "    encoder, decoder = restore_model()\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('Question: ', pair[0])\n",
    "        print('Réponse: ', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words[:-1])\n",
    "        print('Bot: ', output_sentence)\n",
    "        print('-'*50)\n",
    "        \n",
    "def make_dir(path):\n",
    "    \"\"\" Create a directory if there isn't one already. \"\"\"\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        pass\n",
    "def save_model(encoder, decoder):        \n",
    "    make_dir(config.CHECK_POINT_PATH)\n",
    "    path1= os.path.join(config.CHECK_POINT_PATH, 'encoder_70.ck')\n",
    "    path2 = os.path.join(config.CHECK_POINT_PATH, 'decoder_70.ck')\n",
    "    try: \n",
    "        os.remove(path1)  \n",
    "        os.remove(path2)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    torch.save(encoder,path1)\n",
    "    torch.save(decoder, path2)\n",
    "    \n",
    "memo = {}\n",
    "def levenshtein(s, t):\n",
    "    \"\"\"\n",
    "    Pour calculer la distance  Levenshtein entre 2 string s et t\n",
    "    \"\"\"\n",
    "    if s == \"\":\n",
    "        return len(t)\n",
    "    if t == \"\":\n",
    "        return len(s)\n",
    "    cost = 0 if s[-1] == t[-1] else 1\n",
    "       \n",
    "    i1 = (s[:-1], t)\n",
    "    if not i1 in memo:\n",
    "        memo[i1] = levenshtein(*i1)\n",
    "    i2 = (s, t[:-1])\n",
    "    if not i2 in memo:\n",
    "        memo[i2] = levenshtein(*i2)\n",
    "    i3 = (s[:-1], t[:-1])\n",
    "    if not i3 in memo:\n",
    "        memo[i3] = levenshtein(*i3)\n",
    "    res = min([memo[i1]+1, memo[i2]+1, memo[i3]+cost])\n",
    "    \n",
    "    return res\n",
    "    \n",
    "def restore_model():\n",
    "    hidden_size = config.HIDDEN_SIZE\n",
    "    path1 = os.path.join(config.CHECK_POINT_PATH, 'encoder_70.ck')\n",
    "    path2 = os.path.join(config.CHECK_POINT_PATH, 'decoder_70.ck')\n",
    "    if os.path.exists(path1)and os.path.exists(path2):\n",
    "        print('Reading the parameters from {}...'.format(config.CHECK_POINT_PATH))\n",
    "        encoder = torch.load(path1)\n",
    "        decoder = torch.load(path2)\n",
    "    else:\n",
    "        print('Initializing fresh parameters...')\n",
    "        encoder = EncoderRNN(input_lang.n_words, hidden_size)\n",
    "        decoder = AttnDecoderRNN(hidden_size, output_lang.n_words,\n",
    "                               1, dropout_p=0.1)\n",
    "\n",
    "    if use_cuda:\n",
    "        encoder = encoder.cuda()\n",
    "        decoder = decoder.cuda()\n",
    "    return encoder, decoder  \n",
    "def chat():\n",
    "    encoder, decoder = restore_model()\n",
    "    make_dir(config.CHECK_POINT_PATH)\n",
    "    output_file = open(os.path.join(config.CHECK_POINT_PATH, config.OUTPUT_FILE), 'a+')\n",
    "    print('Bonjour, c\\'est le Bot d\\'AVICEN, Je peux vous aider? \\n')\n",
    "    while True:\n",
    "            line = str(input('Vous: '))\n",
    "            if len(line) > 0 and line[-1] == '\\n':\n",
    "                line = line[:-1]\n",
    "            if line == '':\n",
    "                break\n",
    "            line = normalizeString(line)\n",
    "            output_file.write('VOUS ++++ ' + line + '\\n')\n",
    "            reponse, _ = evaluate(encoder, decoder, line)\n",
    "            reponse = \" \".join(reponse[:-1])\n",
    "            output_file.write('BOT ++++ ' + reponse + '\\n')\n",
    "            print('Bot AVICEN: ', reponse)\n",
    "            print('-'*50)\n",
    "    output_file.close()\n",
    "try:\n",
    "    input_lang\n",
    "except NameError : \n",
    "    input_lang, output_lang, pairs = readLangs('questions', 'answers', False)\n",
    "    pairs = TrimWords(pairs) \n",
    "    input_lang, output_lang, pairs = prepareData('questions', 'answers', False)\n",
    "\n",
    "def langTest():\n",
    "    pairs_test = []   \n",
    "    test_file = open(os.path.join(config.PROCESSED_PATH, \"test.txt\"), 'r')\n",
    "    i=0\n",
    "    for line in test_file:\n",
    "        line = normalizeString(line)\n",
    "        if i%2 ==0:\n",
    "            question = line\n",
    "        else:\n",
    "            answer = line\n",
    "            pairs_test.append([question, answer])\n",
    "        i +=1\n",
    "    return pairs_test  \n",
    "\n",
    "def test(pairs_test):\n",
    "    \"\"\"\n",
    "    Use test for know how our model is good\n",
    "    \"\"\"\n",
    "    encoder, decoder = restore_model()\n",
    "    total_loss = 0\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "    for pair in pairs_test:\n",
    "        variable = variablesFromPair(pair)\n",
    "        loss = train( variable[0], variable[1], encoder, decoder, encoder_optimizer, decoder_optimizer,\\\n",
    "                     criterion= criterion)\n",
    "        total_loss +=loss\n",
    "    Length_inputs = len(pairs_test) if len(pairs_test) !=0 else 1\n",
    "    return total_loss/Length_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "expression = '\"([^\"]*)\"'\n",
    "line = 'anh (khong) [xung]la #bien# xanh'\n",
    "print(re.findall(expression, line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "expression = '\"([^\"]*)\"'\n",
    "mot_cles =[]\n",
    "for word  in output_lang.word2index:\n",
    "    if '#' in word:\n",
    "        mot_cles.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mot_cles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the parameters from check_point...\n",
      "Bonjour, c'est le Bot d'AVICEN, Je peux vous aider? \n",
      "\n",
      "Vous: combien de véhicules  sont utilisés aujourd'hui\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:201: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:212: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot AVICEN:  la\n",
      "--------------------------------------------------\n",
      "Vous: combien de véhicules sont roulés\n",
      "Bot AVICEN:  #total_veh_movement#\n",
      "--------------------------------------------------\n",
      "Vous: combien de véhicules sont utilisés\n",
      "Bot AVICEN:  #oui/non#\n",
      "--------------------------------------------------\n",
      "Vous: combien de voitures sont roulées\n",
      "Bot AVICEN:  #total_veh_movement#\n",
      "--------------------------------------------------\n",
      "Vous: combien de voitures sont à l'arrêt\n",
      "Bot AVICEN:  ce véhicule #vin# semble immobilisé\n",
      "--------------------------------------------------\n",
      "Vous: combien de voitures sont immobilisées?\n",
      "Bot AVICEN:  #list_vin_free# véhicules de ce véhicule\n",
      "--------------------------------------------------\n",
      "Vous: combien de véhicules sont immobilisés\n",
      "Bot AVICEN:  #list_vin_free# véhicules du parc sont\n",
      "--------------------------------------------------\n",
      "Vous: est-ce-que tout va bien\n",
      "Bot AVICEN:  ça va et toi\n",
      "--------------------------------------------------\n",
      "Vous: est ce que tout va bien\n",
      "Bot AVICEN:  ça\n",
      "--------------------------------------------------\n",
      "Vous: tout va bien\n",
      "Bot AVICEN:  ça va et toi\n",
      "--------------------------------------------------\n",
      "Vous: il y a des problèmes\n",
      "Bot AVICEN:  le #date#\n",
      "--------------------------------------------------\n",
      "Vous: y a t-il des problèmes\n",
      "Bot AVICEN:  oui le numéro\n",
      "--------------------------------------------------\n",
      "Vous: y a t-il des problèmes\n",
      "Bot AVICEN:  oui le numéro\n",
      "--------------------------------------------------\n",
      "Vous: \n"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the parameters from check_point...\n",
      "Bonjour, c'est le Bot d'AVICEN, Je peux vous aider? \n",
      "\n",
      "Vous: j'ai besoin d'un soutien\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:201: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:212: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot AVICEN:  les inscriptions se font sur #lab_program#\n",
      "--------------------------------------------------\n",
      "Vous: je veux une aide\n",
      "Bot AVICEN:  oui je peux sur ce véhicule en laquelle de sur #lab_program#\n",
      "--------------------------------------------------\n",
      "Vous: je voudrais une aide\n",
      "Bot AVICEN:  oui\n",
      "--------------------------------------------------\n",
      "Vous: comment tu t'appelles\n",
      "Bot AVICEN:  je suis un bot de avicen tu peux m'appeler charlie\n",
      "--------------------------------------------------\n",
      "Vous: vous vous appelez comment\n",
      "Bot AVICEN:  il faut nous contacter par mail ou mieux vous inscrire sur eventbrite\n",
      "--------------------------------------------------\n",
      "Vous: tu t'appelles comments\n",
      "Bot AVICEN:  je suis un bot vous nous contacter par en workshop\n",
      "--------------------------------------------------\n",
      "Vous: tu t'appelles comment\n",
      "Bot AVICEN:  je suis un bot vous nous contacter par en workshop\n",
      "--------------------------------------------------\n",
      "Vous: comment vous vous appelez\n",
      "Bot AVICEN:  selon est nous contacter il est probable qu'un il a été merci de la meilleure en définie\n",
      "--------------------------------------------------\n",
      "Vous: \n"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
